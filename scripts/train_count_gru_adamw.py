"""
Baseline GRU trainer on the count dataset using backprop + AdamW.
Matches the ES model geometry: d_model=d_hidden=64, seq_len=32, vocab=16 by default.
Data: expects train_tokens.npy in data/count (generated by scripts/make_count_data.py).
"""

import argparse
from pathlib import Path

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
import numpy as np


def load_tokens(path: Path):
    return np.memmap(path, mode="r", dtype=np.uint16)


class GRUBlock(nn.Module):
    def __init__(self, d_model: int, d_hidden: int):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        # Manual GRU cell: gates from x and h
        self.gates_x = nn.Linear(d_model, 3 * d_hidden)
        self.gates_h = nn.Linear(d_hidden, 3 * d_hidden)
        self.ln2 = nn.LayerNorm(d_hidden)
        self.mlp = nn.Sequential(
            nn.Linear(d_hidden, 4 * d_hidden),
            nn.ReLU(),
            nn.Linear(4 * d_hidden, d_hidden),
        )

    def __call__(self, x, h):
        x_ln = self.ln1(x)
        gx = self.gates_x(x_ln)
        gh = self.gates_h(h)
        gates = gx + gh
        r, z, n = mx.split(gates, 3, axis=-1)
        r = mx.sigmoid(r)
        z = mx.sigmoid(z)
        n = mx.tanh(n + r * gh[..., : gh.shape[-1] // 3])  # reuse gh part for candidate
        h_new = (1 - z) * n + z * h

        h_res = h_new + x
        h_ln2 = self.ln2(h_res)
        mlp_out = self.mlp(h_ln2)
        return mlp_out + h_res, h_new


class GRUModel(nn.Module):
    def __init__(self, vocab_size: int, seq_len: int, d_model: int, d_hidden: int, layers: int):
        super().__init__()
        self.tok_emb = nn.Embedding(vocab_size, d_model)
        self.pos_emb = mx.array(np.random.normal(0, 0.02, size=(seq_len, d_model)).astype(np.float32))
        self.blocks = [GRUBlock(d_model, d_hidden) for _ in range(layers)]
        self.ln_out = nn.LayerNorm(d_hidden)
        self.head = nn.Linear(d_hidden, vocab_size)

    def __call__(self, x_tokens):
        B, S = x_tokens.shape
        x = self.tok_emb(x_tokens) + self.pos_emb[:S]
        h_states = [mx.zeros((B, self.blocks[0].gates_h.weight.shape[0] // 3), dtype=mx.float32) for _ in self.blocks]
        logits_seq = []
        for t in range(S):
            x_t = x[:, t, :]
            for i, blk in enumerate(self.blocks):
                h_states[i], h_states[i] = blk(x_t, h_states[i])
                x_t = h_states[i]
            logits_t = self.head(self.ln_out(x_t))
            logits_seq.append(logits_t)
        logits = mx.stack(logits_seq, axis=1)  # (B, S, V)
        return logits


def cross_entropy(logits, targets):
    logits_f = logits.astype(mx.float32)
    max_logits = mx.max(logits_f, axis=-1, keepdims=True)
    logsumexp = max_logits + mx.log(mx.sum(mx.exp(logits_f - max_logits), axis=-1, keepdims=True))
    target_logits = mx.take_along_axis(logits_f, targets[..., None], axis=-1)[..., 0]
    return mx.mean(logsumexp[..., 0] - target_logits)


def get_batch(memmap, seq_len: int, batch_size: int, rng):
    total_tokens = memmap.shape[0]
    n_seq = total_tokens // seq_len
    idx = rng.integers(0, n_seq - batch_size)
    arr = np.array(memmap[idx * seq_len : (idx + batch_size) * seq_len], copy=False)
    arr = arr.reshape(batch_size, seq_len)
    x = mx.array(arr[:, :-1].astype(np.int32))
    y = mx.array(arr[:, 1:].astype(np.int32))
    return x, y


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", type=str, default="data/count")
    ap.add_argument("--vocab_size", type=int, default=16)
    ap.add_argument("--seq_len", type=int, default=32)
    ap.add_argument("--d_model", type=int, default=64)
    ap.add_argument("--d_hidden", type=int, default=64)
    ap.add_argument("--layers", type=int, default=1)
    ap.add_argument("--batch_size", type=int, default=64)
    ap.add_argument("--steps", type=int, default=500)
    ap.add_argument("--lr", type=float, default=1e-3)
    ap.add_argument("--weight_decay", type=float, default=0.01)
    ap.add_argument("--seed", type=int, default=0)
    ap.add_argument("--checkpoint", type=str, default=None, help="path to load/save model params")
    ap.add_argument("--prompt", type=str, default=None, help="space-separated numeric tokens for greedy generation")
    ap.add_argument("--max_new_tokens", type=int, default=50)
    args = ap.parse_args()

    rng = np.random.default_rng(args.seed)
    train_path = Path(args.data_dir) / "train_tokens.npy"
    memmap = load_tokens(train_path)

    model = GRUModel(args.vocab_size, args.seq_len, args.d_model, args.d_hidden, args.layers)

    def load_ckpt(path: Path):
        import pickle
        try:
            with open(path, "rb") as f:
                state = pickle.load(f)
            model_state_np = state.get("model", {})
            opt_state_np = state.get("opt", {})
            model_state = {k: mx.array(v) for k, v in model_state_np.items()}
            opt_state = {k: mx.array(v) for k, v in opt_state_np.items()}
        except Exception:
            # Fallback to npz (legacy)
            state = np.load(path, allow_pickle=True)
            model_state = {k.split("model.", 1)[1]: mx.array(np.array(v)) for k, v in state.items() if k.startswith("model.")}
            opt_state = {}
        return model_state, opt_state

    ckpt_path = Path(args.checkpoint) if args.checkpoint else None
    if ckpt_path and ckpt_path.exists():
        print(f"Loading checkpoint from {ckpt_path}...")
        m_state, o_state = load_ckpt(ckpt_path)
        if m_state:
            model.update(m_state)
        if o_state:
            opt.load_state(o_state)
    elif ckpt_path:
        print(f"Checkpoint {ckpt_path} not found, starting fresh.")

    opt = optim.AdamW(learning_rate=args.lr, weight_decay=args.weight_decay)

    # Prompt-only path
    if args.prompt:
        tokens = [int(t) for t in args.prompt.strip().split()]
        seq = list(tokens)
        for _ in range(args.max_new_tokens):
            tail = seq[-args.seq_len:]
            x_tokens = mx.array(np.array(tail, dtype=np.int32))[None, :]
            logits = model(x_tokens)
            next_id = int(mx.argmax(logits[0, -1]).item())
            seq.append(next_id)
        print("Generated:", " ".join(str(i) for i in seq))
        return

    loss_and_grad = mx.value_and_grad(lambda mdl, xb, yb: cross_entropy(mdl(xb), yb))

    for step in range(args.steps):
        x, y = get_batch(memmap, args.seq_len, args.batch_size, rng)
        loss, grads = loss_and_grad(model, x, y)
        opt.update(model, grads)
        if step % 50 == 0:
            print(f"step {step:04d} loss {loss.item():.4f}")

    # Save checkpoint
    if args.checkpoint:
        import pickle
        state = {
            "model": {k: np.array(v) for k, v in model.parameters().items()},
            "opt": {k: np.array(v) for k, v in opt.state.items()},
        }
        with open(args.checkpoint, "wb") as f:
            pickle.dump(state, f)
        print(f"Saved checkpoint to {args.checkpoint}")


if __name__ == "__main__":
    main()
