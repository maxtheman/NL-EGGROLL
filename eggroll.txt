Evolution Strategies at the Hyperscale
Bidipta Sarkar∗1,2, Mattie Fellows∗1, Juan Agustin Duque∗2,3,
Alistair Letcher†1, Antonio León Villares†1, Anya Sims†1, Dylan Cope†1, Jarek Liesen†1,
Lukas Seier†1,Theo Wolf†1, Uljad Berdica†1, Alexander David Goldie1,2,
Aaron Courville3,5, Karin Sevegnani4, Shimon Whiteson‡2, Jakob Nicolaus Foerster‡1
1 FLAIR - University of Oxford, 2 WhiRL - University of Oxford, 3 MILA– Québec AI Institute
4 NVIDIA AI Technology Center, 5 CIFAR AI Chair
{bidipta.sarkar,matthew.fellows,jakob.foerster}@eng.ox.ac.uk
juan.duque@mila.quebec,shimon.whiteson@cs.ox.ac.uk
Abstract
We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL),
an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large
population sizes for modern large neural network architectures with billions of parameters.
ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or
noisy objectives with excellent scaling potential through parallelisation. Naïve ES becomes
prohibitively expensive at scale due to the computational and memory costs associated
with generating matrix perturbations E∈R m×n and the batched matrix multiplications
needed to compute per-member forward passes. EGGROLL overcomes these bottlenecks
by generating random matrices A∈R m×r, B∈R n×r with r≪min(m,n) to form a
low-rank matrix perturbation AB⊤that are used in place of the full-rank perturbation E.
As the overall update is an average across a population of N workers, this still results in
a high-rank update but with significant memory and computation savings, reducing the
auxiliary storage from mn to r(m+n) per layer and the cost of a forward pass from
O(mn) to O(r(m+n)) when compared to full-rank ES. EGGROLL’s efficiency results in a
hundredfold increase in training throughput for billion-parameter models at large population
sizes, nearly reaching the throughput of pure batch inference. A theoretical analysis reveals
our low-rank update converges to the full-rank update at a fast O
(1
r
)
rate. Our experiments
show that (1) EGGROLL does not compromise the performance of ES in tabula-rasa RL
settings, despite being faster, (2) it is competitive with GRPO as a technique for improving
LLM reasoning, and (3) EGGROLL enables stable pre-training of nonlinear recurrent
language models that operate purely in integer datatypes. Code is available at our website:
https://eshyperscale.github.io/
=
 f
(
+σ
)
=
 f
(
+σ
)
... ...
=
 f
(
+σ
)
Initial weights
Rank-one perturbationEi Fitness evaluationWeighted average
Final rank-Nupdate
(a)
Figure 1: Schematic visualization of EGGROLL usingNworkers.
*Equal Contribution † Core Contributor, sorted by alphabetical order in first names ‡Equal Senior Authors
1
arXiv:2511.16652v1  [cs.LG]  20 Nov 2025
EGGROLL PPO OpenES
0
20
40
60
80
100Normalized Speed
91
34
0.41
Normalized Training Speeds
(a)
0 200 400 600 800 1000
Training Step
4
5
6
7
8Loss (bits/byte)
Pure Integer Pretraining: Test Loss
101
102
103
104
105
Population Size (b)
Figure 2: (a) Relative speed of our method, EGGROLL, in terms of experience throughput versus prior methods, where
100 represents the maximum batch throughput of pure inference. See Appendix F for more details. (b) We use EGGROLL
to train RNN language models from scratch using only integer datatypes, scaling population size from 64 to 262144.
1 Introduction
Evolution Strategies (ES) (Rechenberg, 1978; Beyer, 1995; Beyer & Schwefel, 2002) are an attractive
alternative to first-order methods based on gradient backpropagation, for several reasons. First, ES does not
require differentiability, so it can optimise a broader class of models, like models with discrete parametrisation
spaces (cellular automata), and can optimize objectives where gradients are unavailable or noisy, like outcome-
only rewards in LLM fine-tuning (Qiu et al., 2025). Second, ES is more robust to noisy and ill-conditioned
optimisation landscapes (Wierstra et al., 2011; Xue et al., 2021). Unlike gradients, population-based exploration
smooths irregularities (Salimans et al., 2017), tolerates discontinuities, and mitigates issues like ill-conditioned
curvature or vanishing and exploding gradients in long-range or recurrent settings (Hansen, 2023). Third,
ES is highly amenable to scaling through parallelisation, since fitness evaluations are independent across
population members and require only the communication of scalar fitnesses, which maps cleanly onto modern
inference infrastructure and yields near-linear speedups on large clusters (Salimans et al., 2017). By contrast,
backpropagation requires communicating and aggregating gradients across devices, yielding updates with high
memory and computation costs. Additionally, backpropagation requires special care when training models
with low-precision datatypes, whereas ES can directly optimize any model with the same datatypes used
at inference time. Together, these properties position ES as a potentially powerful foundation for training
large, discrete, or hybrid architectures, and end-to-end systems with non-differentiable components, including
large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2023; Du et al., 2022; Fedus et al.,
2022).
Despite this potential, there are practical obstacles to employing ES at scale. In deep learning architectures
(Goodfellow et al., 2016), the majority of trainable parameters form linear mappings represented by matrices
(Rosenblatt, 1962; Hochreiter & Schmidhuber, 1996; Bengio et al., 2000; Krizhevsky et al., 2012; Goodfellow
et al., 2014; Kingma & Welling, 2014; Vaswani et al., 2017); naïvely adapting ES therefore requires generating
full-rank matrix perturbations that replicate the entire parameter set for every population member. This inflates
memory costs and forces frequent movement of large weight tensors. Evaluating these perturbations then
requires a separate sequence of matrix multiplications per member, so the total compute and wall-clock
time scale roughly with the population size and sequence length. In billion-parameter regimes, these two
costs dominate, making it difficult to scale ES beyond small models or small populations (Qiu et al., 2025;
Korotyshova et al., 2025).
To mitigate both memory and computational bottlenecks, we introduce Evolution Guided General Optimization
via Low-rank Learning (EGGROLL), an ES algorithm that allows for the efficient training of neural network
architectures with billions of parameters. Analogous to LoRA’s low-rank adapters in gradient-based training
(Hu et al., 2022), EGGROLL generateslow-rankparameter-space perturbations for ES: instead of sampling
a full-rank matrix E∈R m×n, we sample A∈R m×r and B∈R n×r with r≪min(m,n) and form
E= 1√rAB⊤. This reduces auxiliary perturbation matrix storage from mn to (m+n)r per layer, and
proportionally reduces tensor movement. Moreover, we use a counter-based deterministic random number
generator (RNG) (Salmon et al., 2011; Bradbury et al., 2018) to reconstruct noise on demand, so matrix
2
perturbations need not persist in memory. When evaluating the fitness of members of multiple perturbations
in parallel, EGGROLL batches a population of low-rank adapters and shares the base activations, enabling
a single forward pass that applies all AB⊤updates via specialized batched matrix multiplications. The
compute required beyond standard batched inference scales with O(r(n+m)N) , where N is the population
size, instead of O(nmN) for standard ES, yielding substantial memory and inference savings and making
EGGROLL efficient for extremely large models. We emphasise that our method does not restrict the updates
to be low-rank: the overall EGGROLL update is an average of rank rmatrices across the population, making
the matrix parameter update rankmin(Nr,m,n).
EGGROLL is described in detail in Section 4. We provide a rigorous theoretical analysis of the low-rank
approximation accuracy in Section 5, proving that EGGROLL updates converge to the full rank Gaussian
ES updates at an O(1/r) rate. This fast convergence rate suggests that low-rank updates are sufficient to
train large-scale architectures, even with r≪min(m,n) . In our extensive empirical evaluation, we test this
hypothesis across a wide range of domains in Section 6. In tabula rasa and multi-agent RL (MARL) settings,
we show that EGGROLL does not compromise performance compared to naïve ES despite being faster.
To demonstrate the scalability of our method for LLM fine-tuning, we conduct experiments on pretrained
RWKV7 (Peng et al., 2025) models, modern recurrent language models that enable large batch inference
due to their constant state size. Finally, we develop a nonlinear RNN language model that operates purely in
integer datatypes, and demonstrate that EGGROLL can stably pretrain this language model, a feat which is
only feasible due to the large population sizes enabled by EGGROLL.
2 Preliminaries
All proofs for theoretical results in this paper are found in the Appendix.
2.1 Low Rank Matrix Approximations
When adapting high dimensional foundation models for specific tasks, updating the parameters using gradient
based methods has high memory requirements. LoRA uses low-rank approximations to the matrix multiplica-
tions to reduce these costs. For each matrix Mi ∈R m×n in the model, a low-rank approximation can be made
by decomposing each matrix:
Mi ≈M 0
i +AiB⊤
i ,
where M0
i :=StopGrad(Mi) is the imported matrix from the foundation model with frozen parameters and
Ai ∈R m×r and Bi ∈R n×r are low-width column matrices (i.e., r≪min(m,n) ) whose parameters are
updated through gradient-based optimisation during task-specific adaptation. This reduces the number of
optimisation parameters for each matrix from mnto r(m+n) . In this paper, we use a similar low-rank matrix
approximation for evolutionary strategies.
2.2 Gaussian Matrix Distribution and Matrix Norms
In this paper, we focus on evolution strategies that targetmatrix parameters. Many variables we study are
Gaussian distributed. When working in matrix space, it is convenient to use the matrix Gaussian distribution
(Dawid, 1981), which is defined directly over matricesX∈R m×n:
N(M,U,V) = 1
(2π)
mn
2 det(U)
n
2 det(V)
m
2
exp
(
−1
2tr
(
V−1(X−M) ⊤U−1(X−M)
))
,
where M∈R m×n is the mean matrix, U∈R m×m is the row covariance matrix and V∈R n×n is the
column covariance matrix. The matrix Gaussian distribution is a generalisation of the multivariate Gaussian
distribution N(µ,Σ) defined over vector space. Sampling a matrix X∼N(M,U,V) from a Gaussian matrix
distribution is equivalent to sampling a vector vec(X)∼N(µ,Σ) from a multivariate Gaussian distribution
with mean µ=vec(M) and covariance matrix Σ =V⊗U where ⊗denotes the Kronecker product. For
isotropic matrix Gaussian distributions with covariance matrices U=σ 2Im and V=σ 2In, the equivalent
multivariate Gaussian distribution is also isotropic with Σ =σ2Imn. To measure distance between matrices,
we use the Frobenius norm:
∥M∥F :=
√∑
i,j
mi,j2,
3
which provides an upper bound on the matrix 2-norm (Petersen & Pedersen, 2008). We denote the ℓ2 vector
norm as∥·∥.
2.3 Evolution strategies
Evolution strategies (ES) (Rechenberg, 1978; Beyer, 1995; Beyer & Schwefel, 2002) is a set of blackbox
methods for optimising general systems. ES has emerged as a useful alternative to gradient-based methods,
particularly when a system is noisy or non-differentiable. Our problem setting focuses on fitness functions
whose parameters are matrices: our goal is to find a matrix M⋆ ∈R m×n that maximises the fitness function
M⋆ ∈arg max M∈Rm×n f(M) . In comparison to gradient-based methods which use derivatives of the
function f(M) to update the parameters M directly, evolutionary methods update a population distribution
over the parameter space. This is achieved by learning a set of parameters θto maximise the expected fitness
f(z)under a population distributionπ(M|θ):
J(θ) =E M∼π(M|θ) [f(M)].(1)
Taking derivatives ofJ(θ)yields the gradient:
∇θJ(θ) =E M∼π(M|θ) [∇M logπ(M|θ)f(M)],
which is used to update the population distribution’s parameters using (stochastic) gradient ascent with a
suitable stepsizeαt:
θt+1 ←θt +αt∇θJ(θt).(2)
∇M logπ(M|θ) is known as the score function, which avoids taking gradients directly through the fitness
function in Eq. (1).
2.4 Gaussian Matrix ES
In this paper, we study ES using Gaussian policies: π(M|θ) =N(µ,I mσ2,Inσ2). In addition to its
mathematical convenience, the central limit theorem means that the Gaussian distribution emerges naturally
from a low-rank approximation as rank increases, even if the matrices Aand Bare themselves non-Gaussian.
We assume that σ2 is fixed and the ES algorithm optimises over the mean matrix θ=µ which acts as a proxy
for the true maximum of the fitness function. By adapting well-known derivations (Wierstra et al., 2011) to the
Gaussian matrix case, we derive the ES gradient for our problem setting:
Proposition 1.Using the Gaussian matrix policy π(M|µ) =N(µ,I mσ2,Inσ2) the ES objective can be
written as:
J(µ) =E E∼P(E) [f(M=µ+σE)].(3)
and the matrix gradient of the ES objective is:
∇µJ(µ) =− 1
σEE∼P(E) [E·f(M=µ+σE)].(4)
where P(E) is a zero-mean standard normal p(E) =N(0,I m,In). Moreover, the natural matrix gradient is
equal toσ2∇µJ(µ), i.e. equivalent to Eq.(4)up to a factor ofσ 2.
We see from Eq. (4) that Gaussian matrix ES methods optimise the objective J(µ) by generating search
matrices E∼P(E) from a standard matrix normal distribution N(0,Im,In) around the parameter matrix
θ=µ.
We remark that for Gaussian population distributions, the ES update in Eq. (4) is equal to the natural evolution
stategy (NES) update up to a factor ofσ2. NES (Wierstra et al., 2008; 2011) updates follow thenatural gradient
(Amari, 1998; Kakade, 2001) of the objective in Eq. (1). This means that for our problem setting where σis
assumed fixed and is absorbed into the stepsize in Eq. (2), Gaussian matrix ES and NES are equivalent. A key
benefit of the natural gradient is that it takes into account the local geometry of the underlying parameter space
when updating the search distribution, making the updates invariant to the choice of parametrisation.
4
3 Related Work
3.1 Evolutionary Algorithms
Evolutionary algorithms have long been a compelling alternative to backpropagation-based training methods.
Despite encompassing a broad range of algorithms (e.g., genetic algorithms (Such et al., 2018) or symbolic
evolution (Koza, 1994)), most contemporary research on evolution has shifted towards algorithms that scale
better to large numbers of neural network parameters (Jaderberg et al., 2017; Hansen & Ostermeier, 2001;
Salimans et al., 2017).
Our work focuses on evolving the weights of predetermined architectures, building up from the NES (Wierstra
et al., 2011) family of methods. Such an approach has recently grown in prominence, following the application
of NES to policy learning in conventional RL environments (Salimans et al., 2017) to alleviate some of the
challenges that policy-gradient methods struggle with, like long-horizon environments. Since then, evolution
has been widely applied in other domains, such as meta-learning (e.g., (Lu et al., 2022; Metz et al., 2022; Lange
et al., 2023; Goldie et al., 2024; 2025)), hyperparameter tuning (e.g., (Parker-Holder et al., 2021; Tani et al.,
2021; Vincent & Jidesh, 2023)) and drug discovery (Towers et al., 2025). Here, we consider the limitations
and solutions to applying ES at significant scale, beyond the small networks and population sizes of these prior
works, with a broad focus on policy-learning. In particular, Salimans et al. (2017) uses a maximum population
size of 1440, whereas our maximum population size is on the order of hundreds of thousands.
ES is limited by its requirement for full, potentially expensive evaluations of the fitness function due to
simulating policies in long-horizon environments and potentially high memory usage. Persistent Evolution
Strategies (Vicol et al., 2021) demonstrate a significant speedup by updating networksonline(i.e., during the
unroll), with followups offering further variance reduction (Li et al., 2023b; Vicol et al., 2023). We note that
these works are orthogonal to our focus on scaling up the population size of ES; we leave the application of
these techniques on top of EGGROLL to future work.
3.2 Evolution Strategies for LLMs
Although gradient backpropagation is typically used for LLM training and fine-tuning, prior work has explored
ES variants for fine-tuning. In particular, zeroth order optimization (Zhang et al., 2024), which is analogous to
ES with a population size of 1, is used by Malladi et al. (2023) for memory-efficient LLM fine-tuning. Yu et al.
(2025) extend this approach by projecting perturbations to a low-rank subspace, improving the convergence
of zeroth order optimization. Jin et al. (2024) performs ES directly on LoRA matrices. These works focus
on the supervised fine-tuning setting, finding comparable performance to full fine-tuning, but they do not
determine whether pretraining is possible with zeroth order methods; we find that large population sizes are
necessary for pretraining performance, indicating that zeroth order optimization methods would be unsuitable
for pretraining.
Recent work has also explored ES in the context of LLM reasoning. Korotyshova et al. (2025) first train
LoRA adapters using supervised fine-tuning (SFT) before decomposing them into fixed SVD bases alongside
singular values that are trained using CMA-ES. They achieve comparable performance to GRPO (Shao et al.,
2024) in significantly less wall-clock time on math reasoning benchmarks. Qiu et al. (2025) directly use ES
to optimize all LLM parameters for reasoning, with stronger performance than GRPO on the countdown
reasoning task. However, both of these approaches use relatively small population sizes, on the order of a
hundred unique perturbations per update, and instead collect hundreds of rollouts per perturbation to efficiently
use GPUs. By contrast, our approach allows all generations to use different perturbations, such that our
maximum population size per update is orders of magnitude larger (equal to the maximum inference batch
size), without compromising token generation throughput.
4 EGGROLL
We now introduce and motivate our method EGGROLL, which is presented in Algorithm 1. In Section 4.1
we derive a low-rank ES update that approximates a full-rank ES gradient. One practical issue with using
a low-rank matrix approximation is that its distribution and score function have no analytic solution except
for degenerate cases, so in Section 4.2 we derive an alternative score function from the limiting high-rank
Gaussian which we propose as an approximation.
5
4.1 Low Rank Evolution Strategies Algorithm 1EGGROLL(r,α,σ,T max,Nworkers)
initialiseµand workers with known random seedsς
forT max timestepsdo
forEach workeri∈{1,...N workers}in paralleldo
Ai ∼p(Ai),Bi ∼p(Bi)
Ei ← 1√r AiB⊤
i
fi ←f(µ+σE i)
end for
Workers share scalar fitnessfi with other workers
forEach workeri∈{1,...N workers}in paralleldo
ReconstructEj forj∈{1,...N workers}fromς
µ←µ+α 1
NWorkers
∑NWorkers
j=1 Ejfj
end for
end for
Recall the Gaussian matrix ES update from Eq. (4).
Our goal is to introduce a tractable approximation to
generating full-rank matrices, using low-rank matrices
AB⊤as our search matrices instead. We denote the
distribution of Aas p(A) and Bas p(B) assume that the
elements ofAandBare drawn independently:
Assumption 1.Assume all elements ai,j ∈A and
bi,j ∈B are continuous identically and independently
distributed random variables according to some zero-
mean, symmetric, absolutely continuous (i.e., it has a
density) distribution p0(·) with finite 4th order moments
and variance0<σ 2
0.
The low-rank approximation AB⊤must be carefully integrated into the Gaussian matrix ES objective in
Eq. (3) to ensure that the variance of the updates remains bounded with increasing number of columns ras the
elements of AB⊤are a sum of rrandom variables. To counter this, we scale the outer product by 1√r which
keeps the variance ofE= 1√rAB⊤asO(1).
Observe that E= 1√rAB⊤maps to the manifold Mr ⊂R m×n of rank rmatrices, which is a subset of Rm×n.
This means that the density p(E) is defined with respect to a unit volume over the manifold and cannot be
defined with respect to the standard unit volume in Euclidean space as the volume of the manifold is zero
using this measure. For the corresponding score function, gradients with respect to logp(E) are defined
over the tangent space to Mr instead of in the usual Euclidean space. An intuitive reason for this is that
there is dependence between elements of Eand so it is not possible to take partial derivatives with respect
to each element whilst keeping all other elements free. In principle it is possible to define all distributions
and gradients with respect to the manifold Mr, however this makes an analysis of low-rank updates difficult.
Instead, we study the variable Z= 1√rAB⊤+ϵ where ϵis an m×n matrix with independent decaying
Gaussian elementsϵi,j ∼N(0, σ2
ϵ/r). The variableZ= 1√rAB⊤+ϵhas a well-defined densityp(Z)for all
σϵ >0 . This conceptual tool greatly simplifies analysis and asσϵ can be made arbitrarily small and hencep(E)
can be arbitrarily close to p(Z), the difference between the update used in practice and the update analysed is
negligible. Substituting forp(E) =p(Z)in Eq. (3), we derive the low-rank ES objective:
JLR(µ) =E Z∼p(Z) [f(M=µ+σZ)].
We now verify thatp(z) is well-defined and show how JLR(µ) can be optimised using the score function of
p(Z):
Theorem 1.Low-rank ES Gradient.Let Assumption 1 hold. Then for finiter:
gLR :=∇µJLR(µ) =−1
σEZ∼p(Z) [∇Z logp(Z)f(M=µ+σZ)].
where Z is generated by sampling A∼p(A),B∼p(B),ϵ∼p(ϵ) and making the transformation Z=
1√rAB⊤+ϵ.
4.2 Score Function Approximation
As the mapping (A,B,ϵ) :→Z= 1√rAB⊤+ϵ is non-invertible, there does not exist a simple closed form
solution for the density p(Z) and score function except in degenerate cases such as m=n= 1 . Instead, we
use an approximation for the score function ˆS(Z)≈∇ Z logp(Z):
ˆgLR =−1
σEZ∼p(Z)
[
ˆS(Z)f(M=µ+σZ)
]
.(5)
To optimise the ES objective using the EGGROLL update, we adapt the parallelised evolutionary strategies
algorithm from Salimans et al. (2017). In our experiments and Algorithm 1, we use a Gaussian approximate
6
score function, which is obtained from taking the limitr→∞ . As ϵis a random matrix formed of independent
Gaussian elements, it can be decomposed into an equivalent sum of rindependent Gaussian matrices:
ϵ= 1√r
r∑
i=1
ϵi,
where each random matrix ϵi ∼p(ϵ i) has the same distribution as ϵ. Likewise, the matrix AB⊤can be
decomposed as a sum of independent, zero-mean vector outer products:
AB⊤=
r∑
i=1
aib⊤
i ,
where ai and bi are the ith column vectors of Aand B. This allows us to write Zas a standardised sum of r
independent random matrices:
Z= 1√r
r∑
i=1
(
aib⊤
i +ϵi
)
.(6)
Under Assumption 1, the central limit theorem proves that p(Z) converges in distribution to a Gaussian
N(0,Imσ4
0,Inσ4
0). Using this limiting distribution in place of the true distribution p(Z), we obtain the
Gaussian approximate score function:
ˆS(Z) =− 1
σ4
0
Z.(7)
We remark that EGGROLL is not wedded to any particular score function approximator and we derive and
explore a set of mean-field approximators in Appendix B as alternatives, however our experiments show that
the Gaussian approximator has the best overall performance on the set of tasks we evaluated on. We make
a Monte Carlo estimate of expectation in Eq. (5) with Nworkers samples to optimise the parameters µusing
(approximate) stochastic gradient ascent. This yields the Gaussian EGGROLL update:
EGGROLL UPDATE:For each worker (in parallel), sample Ai,t ∼p(Ai,t),Bi,t ∼p(Bi,t) and form
low-rank perturbationEi,t = 1√rAi,tB⊤
i,t. Update matrix parameters using:
µt+1 =µt + αt
Nworkers
Nworkers∑
i=1
Ei,tf(M=µ t +σEi,t).(8)
Here we have absorbed the constants 1
σ and 1
σ04 into the tunable learning rate αt. As each random matrix
Ei,t in Eq. (8) has rank r almost surely and the matrix is updated using a sum of Nworker such matrices,
the overall EGGROLL matrix parameter update has rank min(Nr,m,n) almost surely, meaning that the
overall parameter update is not restricted to be low-rank. For all experiments in Section 6, Nr>min(,m,n) ,
meaning EGGROLL parameter updates are full-rank.
4.3 Hardware-Efficient EGGROLL Implementation
A key reason to use EGGROLL over standard ES is that large populations can be simulated in parallel on a
GPU thanks to the low-rank perturbations. For the sake of exposition, we write equations from the perspective
of a single worker,i, and explain in text how this corresponds to batched GPU operations.
Consider the task of computing a batched forward pass over inputs xi ∈R din for a linear layer with mean
parameter µ∈R dout×din. The standard forward pass is just a regular matrix multiplication, xµT, since µis
constant across all threads. However, naively applying ES by trying to compute xi(µ+σE i)T becomes a
batched matrix multiplication, which is inefficient on GPUs since every element of µ+σE i is only used in a
single multiplication, yielding poor arithmetic intensity.
7
However, with EGGROLL we know that xi(µ+σE i) =x iµ+ σ√r(xiBi)AT
i . In this context, the bulk of
compute is spent on the efficient calculation of xiµusing regular matrix multiplication. Meanwhile, when
r= 1 , xiBi simply becomes an inexpensive batched vector-vector dot product to get a batch of scalars, which
is then processed by a batched scalar-vector multiplication when multiplying by AT
i . This decomposition
is key to efficient batched LoRA inference, such as those used by vLLM (Kwon et al., 2023), which why
EGGROLL achieves the same speeds as batched LoRA inference systems.
We additionally optimize the update process by not explicitly materializing the individualEiin the computation
of ∑N
i=1 Eifi, the key term in the Gaussian approximate score function. In particular, when the rank is 1, we
reconstruct A∈R N×dout and B∈R N×din and calculate the expression as (A⊙f)B T, which is a simple
matrix multiplication.
5 Approximation Analysis
We now analyse how fast the Gaussian score approximation from Eq. (7) converges to the true Gaussian
ES matrix gradient in Eq. (4). We introduce the following formal regularity assumption for the fitness
function:
Assumption 2.Assume thatf(M)is bounded, that issup M |f(M)|<∞.
Denote the true full-rank Gaussian ES gradient as gTrue :=∇µJ(µ). Our key theoretical result characterises
the error rate between the Gaussian score approximator in the low-rank update ˆgr
LR from Eq. (5) and the true
gradient using the matrix Frobenius norm:
Theorem 2.Let Assumptions 1 and 2 hold and setσ 0 = 1, then:
∥ˆgr
LR −gTrue∥F =O
(1
r
)
.(9)
3
 2
 1
 0 1 2 3
zi, j
0.4
0.2
0.0
0.2
0.4
p(zi, j)zi, j 
r=1
r=2
r=3
r=5
r=10
r=50
r=100
r
Figure 3: Plot of Marginal Score Multiplied by Density for
Increasingr
The convergence rate in Eq. (9) is faster than the
typical O( 1/√r) rate dictated by the general para-
metric central limit theorem. Our analysis shows that
this is due to the symmetry in our problem under
Assumption 1. To obtain our results, we make an
Edgeworth expansion (Bhattacharya & Ranga Rao,
1976) of the density p(Zr), which expands p(Zr)
as the limiting Gaussian distribution plus a sum of
decaying terms that are controlled by the 3rd order
and higher cumulants of p(Zr). Each ith order cu-
mulant term is multiplied by a factor that decays at
rate O
(
r−i−2
2
)
. For symmetric zero-mean distribu-
tions, all odd cumulants are zero (for the same reason
that all odd moments of a symmetric distribution are
zero). Hence, the rate of convergence to the limiting distribution is controlled by the 4th order term, which has
rateO
(
r−1)
.
Although the full distribution p(Zr) has no general closed-form solution, the distribution over marginals
p(zi,j) is more amenable to analysis. We derive p(zi,j) for generalised Gaussian distributed ai,j and bi,j in
Section B. To illustrate the fast converge rate, we plot the negative density×score function p(zi,j)zi,j for the
marginal distribution p(zi,j) in Fig. 3 using Gaussian distributed ai,j and bi,j with σ2
0 = 1 (see Theorem 4 for
a derivation). The figure shows that p(zi,j)zi,j quickly converges to the limiting function zi,j√
2π exp
(
−zi,j
2
2
)
,
recovering the Gaussian form from the true natural policy gradient update. Even at r= 1 , the function is not a
poor approximation. After r= 10 , the function has nearly converged and after r= 50 , the function is visually
indistinguishable from the limit, providing evidence for the hypothesis that the low-rank approximation is
accurate even for very low rank regimesr≪min(m,n).
8
6 Experiments
6.1 Pure Integer Pretraining of an RNN Language Model
To demonstrate the potential of EGGROLL as a general optimization method, we study whether EGGROLL
could be used for language model pretraining. Since EGGROLL does not rely on gradients, we can explicitly
design a language model architecture to be efficient and hardware-friendly at inference time. In particular, we
build a model under the following constraints to emphasize the flexibility of EGGROLL:
1. Pure Integer Training:On H100 systems, int8 is the fastest datatype, with int8 matrix multiplication
with int32 accumulation being the fastest tensor core operation. Furthermore, integer datatypes are
much simpler to implement in hardware, providing massive energy savings for high-throughput
systems (Horowitz, 2014). Therefore, we keep all weights in int8 and all activations in integer
formats,nevercasting to floating point at any point during training.
2. Nonlinear RNN:Modern language models use sequence-parallel architectures like Transformers and
SSMs, since they enable stable gradients without backpropagation through time. However, most of
these sequence-parallel architectures are unable to handle simple state tracking (Merrill et al., 2024),
whereas classic recurrent networks like LSTMs and GRUs can handle these problems with a single
layer. Since EGGROLL does not require backpropagation through time, we can train on unbounded
sequence lengths (Li et al., 2023a) with nonlinear RNNs of broader complexity classes. Specifically,
we develop a variant of the minGRU model (Heck & Salem, 2017) that performs all operations in
integer formats.
3. Removal of all Activation Functions:Inspired by Foerster (2017), we remove all activation
functions, like the rectified linear unit and hyperbolic tangent, due to the nonlinearity present in the
int8 datatype. Specifically, the saturated addition of int8 values provides sufficient nonlinearity due to
the implicit clipping of values to the int8 dynamic range, which evolution strategies can exploit.
We call the resulting language model EGG, theEvolvedGenerativeGRU, an EGGROLL-friendly architecture.
Its architecture is similar to standard pre-layernorm transformer decoder models, but we (1) use a variant of L1
normalization instead of L2 normalization for our layernorm to avoid square roots, (2) replace self-attention
with our custom GRU, and (3) perform all operations in integer datatypes. See Appendix C for more details on
the architecture.
We train an EGG model with 6 layers and hidden dimension 256 to do character-level prediction on the
minipile dataset (Kaddour, 2023). We update parameters after 100 tokens for each population member,
applying truncated ES by keeping the hidden state and only resetting at document boundaries. We plot the
test loss in Fig. 2b over training steps across a range of population sizes, where the best test loss is 3.41
bits/byte. We find that training is stable and loss curves are relatively smooth, especially with large population
sizes, avoiding loss spikes, nan values, and other instabilities associated with backprop-based training at low
precision datatypes.
Note that our largest population size is 218 = 262144, which is two orders of magnitude larger than the largest
experiment done by Salimans et al. (2017) while only requiring a single GPU to train. We see that multiplying
the population size by 8 results in the loss dropping by approximately 0.4 over the range of population values
we have tested, though this pattern will eventually break since the loss must be strictly positive. We conduct
more ablations in Appendix E, determining how data efficient training can be achieved with EGGROLL and
validating the importance of large batch sizes.
6.2 Reinforcement Learning Tasks
In these experiments, we compare the performance of EGGROLL against standard OpenES as implemented in
Salimans et al. (2017) on reinforcement learning tasks. Given the small network sizes, we can use Open ES at
this scale, but we note that as network sizes increase, the use of vanilla OpenES becomes impossible. We use
the standard formulation of simply optimizing for the final return in the environment. For both EGGROLL
and OpenES, we perform hyperparameter optimization (HPO) separately for each environment. For each
algorithm–environment pair, we define plausible ranges for all key hyperparameters based on prior work and
preliminary experiments. We then perform 20 random search trials, where each trial corresponds to a single
9
0 2 4
Steps 1e8
0.0
0.5
1.0Normalized Return
Pendulum-v1
EggRoll
OpenES
PPO
0 2 4
Steps 1e8
0.0
0.5
1.0Normalized Return
Brax Inverted Double Pendulum
0 2 4
Steps 1e8
0.0
0.5
1.0Normalized Return
Craftax Symbolic
0 2 4
Steps 1e8
0.0
0.5
1.0Normalized Return
Jumanji 2048
0 2 4
Steps 1e8
0.0
0.5
1.0
1.5Normalized Return
Kinetix Thrust Control Left (m)
0 2 4
Steps 1e8
0.0
0.5
1.0Normalized Return
Navix DoorKey (8x8)
Figure 4: Comparison of reinforcement learning Mean returns normalized by PPO performance for 10 seeds. The returns
are evaluated using the mean of the parameters. HPO was conducted for each algorithm/environment pair. The shaded
region is the standard error of the mean .
training run with a randomly sampled hyperparameter configuration. Each configuration is evaluated based on
the final return achieved by the mean policy parameters at the end of training. After all trials, we select the
configuration that yields the highest final return. Using this best configuration, we then run 10 independent
seeds to evaluate performance and report the mean and standard error of the mean across these seeds.
We use policy networks with 3 layers of 256 neurons and a range of environments that demonstrate different
capabilities. We evaluate across the Navix (Pignatelli et al., 2024), Craftax (Matthews et al., 2024), Brax
(Freeman et al., 2021), Kinetix (Matthews et al., 2025), and Jumanji (Bonnet et al., 2024) suites of environments.
We evaluate 16 environments in total. To pick environments, we choose environments that are not trivial or
impossible for PPO to solve, according to the original papers. We also choose environments that are part of
different categories when these are available (e.g. environment size in Kinetix or categories in Jumanji).
We show a subsample of the environments that were evaluated in Fig. 4. The remaining environment results
are in Appendix G.1. Our findings show that EGGROLL is competitive with Open ES on 7/16 environments,
underperforms on 2/16, and outperforms on 7/16. This does not take into account the speed-ups when
compared to using OpenES (full-rank updates). We postulate that the reason for this performance increase is
that the large networks are difficult to optimize for Open ES and lend themselves well to low rank updates. All
hyperparameter configuration details are available in Appendix G.1.
6.3 LLM Fine-tuning for Reasoning Tasks
We apply EGGROLL for LLM finetuning of RWKV-7 (Peng et al., 2025) models in two reasoning tasks:
countdown and GSM8K. The RWKV architecture is a recurrent model that, compared to transformers, is
especially suited to parallelization due to the fact that any memory otherwise spent on the KV cache can be
used to evaluate population members. The training curves of EGGROLL and GRPO in countdown are shown
in figure 5a. EGGROLL fine-tuning on an RWKV-7 1.5B model converges to a higher validation accuracy
of 35% (v.s. 23%) under the same hardware and wall-clock time in the countdown task. Similarly, figure 5b
10
0 1 2 3 4 5 6 7 8
Relative wall-clock time (hours)
0.0
0.1
0.2
0.3Validation Score
Countdown — RWKV 7g1.5B
GRPO (n=3) EGGROLL (n=3)
(a)
0 2 4 6 8 10
Relative wall-clock time (hours)
0.60
0.65
0.70
0.75
0.80Validation Score
GSM8K — RWKV 7g7B
GRPO (n=3) EGGROLL (n=3) (b)
Figure 5: Comparison of the validation score of 3 seeds of EGGROLL v.s. 3 seeds of GRPO in: (a) Countdown task with
an RWKV 7g1.5B model on a single GPU. EGGROLL allows 1024 parallel generations per GPU whereas GRPO only 32.
(b) GSM8K task with an RWKV 7g7B model on 8 GPUs. EGGROLL allows 8096 parallel genrations (1024 per GPU)
whereas GRPO only 256 (32 per GPU).
shows that EGGROLL outperforms GRPO on GSM8K fine-tuning. Our scoring function draws parallels to the
group relative advantage of GRPO. In particular, to score a set of noise directions,E≡{E 1,...,E n}, we first
compute their accuracies, {s1,qi,...,s n,qi}, on |q|=m questions, creating a matrix of scores S∈R m×n.
We then compute the zscore per question, with the main difference that we use the global variance ¯σ, and
average over all the questions to compute the final score for the noise directionEi:
¯si = 1
m
m∑
j=1
zi,qj = 1
m
m∑
j=1
si,j −µqj
¯σ .
This scoring function has the purpose of weighting all questions within the same batch the same across
population members.
7 Conclusion
In this paper, we introduce EGGROLL, a powerful method for blackbox optimisation that scales evolutionary
strategies to billion-parameter models and beyond using low-rank search matrices. Our experiments demon-
strate that EGGROLL is effective with rank as small as r= 1 , which represents substantial computational and
memory savings for negligible decrease in performance when compared to the full-rank ES update. Empirically,
EGGROLL delivers large speedups over naïve ES in tabula rasa and multi-agent RL, and can power end-to-end
training pipelines for large language models. Our theoretical analysis reveals that the low-rank EGGROLL
update quickly converges with rank r, however further theoretical analysis is needed to explain the success of
our method whenr= 1.
Looking forward, we are working on applying EGGROLL for other problems beyond the reach of modern
gradient-based techniques. In particular, EGGROLL can enable the training of large scale end-to-end neu-
rosymbolic systems (Sarker et al., 2021) which have nondifferentiable components. For instance, we can
train neural networks that directly interface with symbolic modules for specialized functions, like memory
or calculations. We can also optimize end-to-end systems of language models, training them to be aware of
inference-time harnesses and interactions with other agents in complex systems.
Acknowledgements
Compute for this project is graciously provided by the Isambard-AI National AI Research Resource, under
the projects “FLAIR 2025 Moonshot Projects” and “Robustness via Self-Play RL.” Some experiments also
used compute generously given by JASMIN, the UK’s collaborative data analysis environment ( https:
//www.jasmin.ac.uk).
11
Bidipta Sarkar is supported by the Clarendon Fund Scholarship in partnership with a Department of Engineering
Science Studentship for his Oxford DPhil. Mattie Fellows is funded by a generous grant from the UKRI
Engineering and Physical Sciences Research Council EP/Y028481/1. Jakob Nicolaus Foerster is partially
funded by the UKRI grant EP/Y028481/1 (originally selected for funding by the ERC). Jakob Nicolaus Foerster
is also supported by the JPMC Research Award and the Amazon Research Award. Juan Agustin Duque is
supported by the St-Pierre-Larochelle Scholarship at the University of Montreal and by Aaron Courville’s
CIFAR AI Chair in Representations that Generalize Systematically. Lukas Seier is supported by the Intelligent
Earth CDT with funding from the UKRI grant number EP/Y030907/1.
References
Shun-ichi Amari. Natural gradient works efficiently in learning.Neural Computation, 10(2):251–276, 1998.
doi: 10.1162/089976698300017746.
A. B. Basset.A Treatise on Hydrodynamics: with numerous examples, volume 2. Deighton, Bell, and Co.,
Cambridge, UK, 1892.
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. In T. Leen,
T. Dietterich, and V . Tresp (eds.),Advances in Neural Information Processing Systems, volume 13.
MIT Press, 2000. URL https://proceedings.neurips.cc/paper_files/paper/2000/
file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf.
Hans-Georg Beyer. Toward a theory of evolution strategies: Self-adaptation.Evolutionary Computation, 3:
311–347, 1995. URLhttps://api.semanticscholar.org/CorpusID:17416734.
Hans-Georg Beyer and Hans-Paul Schwefel. Evolution strategies –a comprehensive introduction.Natural
Computing, 1(1):3–52, 2002.
R. N. Bhattacharya and R. Ranga Rao.Normal approximation and asymptotic expansions. Wiley series in
probability and mathematical statistics. Wiley, New York, 1976. ISBN 047107201X.
Clément Bonnet, Daniel Luo, Donal Byrne, Shikha Surana, Sasha Abramowitz, Paul Duckworth, Vincent
Coyette, Laurence I. Midgley, Elshadai Tegegn, Tristan Kalloniatis, Omayma Mahjoub, Matthew Macfarlane,
Andries P. Smit, Nathan Grinsztajn, Raphael Boige, Cemlyn N. Waters, Mohamed A. Mimouni, Ulrich
A. Mbou Sob, Ruan de Kock, Siddarth Singh, Daniel Furelos-Blanco, Victor Le, Arnu Pretorius, and
Alexandre Laterre. Jumanji: a diverse suite of scalable reinforcement learning environments in jax, 2024.
URLhttps://arxiv.org/abs/2306.09884.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George
Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable
transformations of Python+NumPy programs, 2018. URLhttp://github.com/jax-ml/jax.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners, 2020. URLhttps://arxiv.org/abs/2005.14165.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, . . . , Barret Zoph, Liam Fedus, and . . .
(full list). Palm: Scaling language modeling with pathways.J. Mach. Learn. Res., 24(1144), 2023. URL
https://jmlr.org/papers/volume24/22-1144/22-1144.pdf.
A. P. Dawid. Some matrix-variate distribution theory: Notational considerations and a bayesian application.
Biometrika, 68(1):265–274, 1981. ISSN 0006-3444.
12
Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou,
Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju
Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient
scaling of language models with mixture-of-experts. InProceedings of the 39th International Conference
on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pp. 5547–5569, Jul 2022.
URLhttps://proceedings.mlr.press/v162/du22c.html.
Richard Durrett.Probability: Theory and Examples. Cambridge University Press, Cambridge, UK, 4th edition,
2010.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: scaling to trillion parameter models
with simple and efficient sparsity.J. Mach. Learn. Res., 23(1):1–39, January 2022. ISSN 1532-4435. URL
https://jmlr.org/papers/volume23/21-0998/21-0998.pdf.
Jakob Nicolaus Foerster. Nonlinear computation in deep linear networks, sep 2017. URLhttps://blog.
openai.com/nonlinear-computation-in-linear-networks/. Accessed: 2025-11-20.
Gerald B. Folland.Real Analysis: Modern Techniques and Their Applications. John Wiley & Sons, New York,
2nd edition, 1999. See Theorem 8.22 (Riemann–Lebesgue Lemma).
C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. Brax – a
differentiable physics engine for large scale rigid body simulation, 2021. URL https://arxiv.org/
abs/2106.13281.
Alexander D. Goldie, Chris Lu, Matthew T. Jackson, Shimon Whiteson, and Jakob N. Foerster. Can Learned
Optimization Make Reinforcement Learning Less Difficult? InAdvances in Neural Information Processing
Systems, volume 37, pp. 5454–5497, 2024.
Alexander David Goldie, Zilin Wang, Jaron Cohen, Jakob Nicolaus Foerster, and Shimon Whiteson. How
Should We Meta-Learn Reinforcement Learning Algorithms? May 2025. URL https://openreview.
net/forum?id=jKzQ6af2DU.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.Deep Learning. MIT Press, 2016. http://www.
deeplearningbook.org.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes,
N. Lawrence, and K.Q. Weinberger (eds.),Advances in Neural Information Processing Systems, vol-
ume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_
files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf.
I. S. (Izrail Solomonovich) Gradshte˘ın, I. M. (Iosif Moiseevich) Ryzhik, Daniel Zwillinger, Victor Moll, and
Inc Scripta Technica.Table of integrals, series, and products. Academic Press, San Diego ; Tokyo, 8 edition,
2015. ISBN 0123849330.
G R Grimmett and D R Stirzaker. Probability and random processes.Journal of the Royal Statistical Society.
Series A, Statistics in society, 156(3):503–503, 1993. ISSN 0964-1998.
Peter Hall.The bootstrap and Edgeworth expansion. Springer series in statistics. Springer-Verlag, New York,
1992. ISBN 9780387945088.
Nikolaus Hansen. The cma evolution strategy: A tutorial, 2023. URL https://arxiv.org/abs/1604.
00772.
Nikolaus Hansen and Andreas Ostermeier. Completely Derandomized Self-Adaptation in Evolution Strategies.
Evolutionary Computation, 9(2):159–195, June 2001. ISSN 1063-6560. doi: 10.1162/106365601750190398.
URLhttps://ieeexplore.ieee.org/document/6790628.
13
Joel Heck and Fathi M. Salem. Simplified minimal gated unit variations for recurrent neural networks, 2017.
URLhttps://arxiv.org/abs/1701.03452.
Sepp Hochreiter and Jürgen Schmidhuber. Lstm can solve hard long time lag problems. In M.C. Mozer,
M. Jordan, and T. Petsche (eds.),Advances in Neural Information Processing Systems, volume 9.
MIT Press, 1996. URL https://proceedings.neurips.cc/paper_files/paper/1996/
file/a4d2f0d23dcc84ce983ff9157f8b7f88-Paper.pdf.
Mark Horowitz. 1.1 computing’s energy problem (and what we can do about it). In2014 IEEE International
Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pp. 10–14, 2014. doi: 10.1109/ISSCC.
2014.6757323.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. Lora: Low-rank adaptation of large language models. InICLR. OpenReview.net, 2022.
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki, Jeff Donahue, Ali Razavi,
Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray Kavukcuoglu.
Population Based Training of Neural Networks, November 2017. URL http://arxiv.org/abs/
1711.09846. arXiv:1711.09846 [cs].
Feihu Jin, Yifan Liu, and Ying Tan. Derivative-free optimization for low-rank adaptation in large language
models.IEEE/ACM Trans. Audio, Speech and Lang. Proc., 32:4607–4616, October 2024. ISSN 2329-9290.
doi: 10.1109/TASLP.2024.3477330. URL https://doi.org/10.1109/TASLP.2024.3477330.
Jean Kaddour. The minipile challenge for data-efficient language models.arXiv preprint arXiv:2304.08442,
2023.
Sham M Kakade. A natural policy gradient. In T. Dietterich, S. Becker, and Z. Ghahra-
mani (eds.),Advances in Neural Information Processing Systems, volume 14. MIT Press,
2001. URL https://proceedings.neurips.cc/paper_files/paper/2001/file/
4b86abe48d358ecf194c56c69108433e-Paper.pdf.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun
(eds.),2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April
14-16, 2014, Conference Track Proceedings, 2014. URLhttp://arxiv.org/abs/1312.6114.
Daria Korotyshova, Boris Shaposhnikov, Alexey Malakhov, Alexey Khokhulin, Nikita Surnachev, Kirill
Ovcharenko, George Bredis, Alexey Gorbatovski, Viacheslav Sinii, and Daniil Gavrilov. Essa: Evolutionary
strategies for scalable alignment, 2025. URLhttps://arxiv.org/abs/2507.04453.
John R. Koza. Genetic programming as a means for programming computers by natural selection.Statistics
and Computing, 4(2):87–112, June 1994. ISSN 1573-1375. doi: 10.1007/BF00175355. URL https:
//doi.org/10.1007/BF00175355.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger
(eds.),Advances in Neural Information Processing Systems, volume 25. Curran Associates,
Inc., 2012. URL https://proceedings.neurips.cc/paper_files/paper/2012/file/
c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gon-
zalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with
pagedattention. InProceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles,
2023.
Robert Tjarko Lange, Tom Schaul, Yutian Chen, Tom Zahavy, Valentin Dallibard, Chris Lu, Satinder Singh,
and Sebastian Flennerhag. Discovering Evolution Strategies via Meta-Black-Box Optimization, March
2023. URLhttp://arxiv.org/abs/2211.11260. arXiv:2211.11260 [cs].
14
Pierre-Simon Laplace. Mémoire sur les intégrales définies et leur application aux probabilités, et spécialement
à la recherche du milieu qu’il faut choisir entre les résultats des observations.Mémoires de la Classe des
Sciences Mathématiques et Physiques de l’Institut Impérial de France,1 re série, 11(1re partie):297–347,
1811.
V . P. Leonov and A. N. Shiryaev. On a method of calculation of semi-invariants.Theory of Probability and Its
Applications, 4(3):342–355, 1959. URLhttps://www.mathnet.ru/eng/tvp4894.
Oscar Li, James Harrison, Jascha Sohl-Dickstein, Virginia Smith, and Luke Metz. Variance-reduced gradient
estimation via noise-reuse in online evolution strategies. InThirty-seventh Conference on Neural Information
Processing Systems, 2023a.
Oscar Li, James Harrison, Jascha Sohl-Dickstein, Virginia Smith, and Luke Metz. Noise-Reuse in Online
Evolution Strategies, April 2023b. URL http://arxiv.org/abs/2304.12180. arXiv:2304.12180
[cs].
Jarek Liesen, Chris Lu, and Robert Lange. rejax, 2024. URL https://github.com/keraJLi/rejax.
Ryan Lowe, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for
mixed cooperative-competitive environments.Advances in neural information processing systems, 30, 2017.
Chris Lu, Jakub Kuba, Alistair Letcher, Luke Metz, Christian Schroeder de Witt, and Jakob Foerster. Discov-
ered policy optimisation.Advances in Neural Information Processing Systems, 35:16455–16468, 2022.
H. M. Macdonald. Zeroes of the bessel functions.Proceedings of the London Mathematical Society, 30:
165–179, 1899. doi: 10.1112/plms/s1-30.1.165.
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora.
Fine-tuning language models with just forward passes. InProceedings of the 37th International Conference
on Neural Information Processing Systems, NIPS ’23, Red Hook, NY , USA, 2023. Curran Associates Inc.
Michael Matthews, Michael Beukman, Benjamin Ellis, Mikayel Samvelyan, Matthew Jackson, Samuel
Coward, and Jakob Foerster. Craftax: A lightning-fast benchmark for open-ended reinforcement learning.
arXiv preprint arXiv:2402.16801, 2024.
Michael T. Matthews, Michael Beukman, Chris Lu, and Jakob Nicolaus Foerster. Kinetix: Investigating
the training of general agents through open-ended physics-based control tasks. InICLR, 2025. URL
https://openreview.net/forum?id=zCxGCdzreM.
P. McCullagh. Cumulants (lecture notes). Lecture notes, University of Chicago (pdf), 2013. URL https:
//www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf.
Peter McCullagh.Tensor Methods in Statistics. Monographs on Statistics and Applied Probability. Chapman
& Hall / CRC, London, 1987. URL https://www.stat.uchicago.edu/~pmcc/tensorbook/
DoverEdition.pdf. Discusses multivariate moments and cumulants (cumulant tensors).
William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. In
Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and
Felix Berkenkamp (eds.),Proceedings of the 41st International Conference on Machine Learning, volume
235 ofProceedings of Machine Learning Research, pp. 35492–35506. PMLR, 21–27 Jul 2024. URL
https://proceedings.mlr.press/v235/merrill24a.html.
Luke Metz, James Harrison, C. Daniel Freeman, Amil Merchant, Lucas Beyer, James Bradbury, Naman
Agrawal, Ben Poole, Igor Mordatch, Adam Roberts, and Jascha Sohl-Dickstein. VeLO: Training Versatile
Learned Optimizers by Scaling Up, November 2022. URL http://arxiv.org/abs/2211.09760.
arXiv:2211.09760 [cs, math, stat].
Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably Efficient Online Hyperparameter Opti-
mization with Population-Based Bandits, June 2021. URL http://arxiv.org/abs/2002.02518.
arXiv:2002.02518 [cs].
15
Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing
Liu, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, Nathan Wilce, Johan S. Wind,
Tianyi Wu, Daniel Wuttke, and Christian Zhou-Zheng. Rwkv-7 "goose" with expressive dynamic state
evolution, 2025. URLhttps://arxiv.org/abs/2503.14456.
K. B. Petersen and M. S. Pedersen. The matrix cookbook, October 2008. URL http://www2.imm.dtu.
dk/pubdb/p.php?3274. Version 20081110.
Eduardo Pignatelli, Jarek Liesen, Robert Tjarko Lange, Chris Lu, Pablo Samuel Castro, and Laura Toni. Navix:
Scaling minigrid environments with jax, 2024. URLhttps://arxiv.org/abs/2407.19396.
Xin Qiu, Yulu Gan, Conor F. Hayes, Qiyao Liang, Elliot Meyerson, Babak Hodjat, and Risto Miikkulainen.
Evolution strategies at scale: Llm fine-tuning beyond reinforcement learning, 2025. URL https://
arxiv.org/abs/2509.24372.
I. Rechenberg. Evolutionsstrategien. In Berthold Schneider and Ulrich Ranft (eds.),Simulationsmethoden
in der Medizin und Biologie, pp. 83–114, Berlin, Heidelberg, 1978. Springer Berlin Heidelberg. ISBN
978-3-642-81283-5.
V . K. Rohatgi.An introduction to probability theory and mathematical statistics. Wiley series in probability
and mathematical statistics. Wiley, New York, 1976. ISBN 0471731358.
Frank. Rosenblatt.Principles of neurodynamics : perceptrons and the theory of brain mechanisms.Spartan
Books, Washington, 1962.
Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson,
Timon Willi, Akbir Khan, Christian Schroeder de Witt, Alexandra Souly, et al. Jaxmarl: Multi-agent rl
environments in jax.arXiv preprint arXiv:2311.10090, 2023.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable
alternative to reinforcement learning, 2017. URLhttps://arxiv.org/abs/1703.03864.
John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw. Parallel random numbers: As easy as 1, 2,
3. InSC ’11: Proceedings of 2011 International Conference for High Performance Computing, Networking,
Storage and Analysis, pp. 1–12, 2011. doi: 10.1145/2063384.2063405.
Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. Neuro-symbolic artificial intelligence:
Current trends, 2021. URLhttps://arxiv.org/abs/2105.05330.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,
Y . K. Li, Y . Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open
language models, 2024. URLhttps://arxiv.org/abs/2402.03300.
Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning
algorithms, 2012. URLhttps://arxiv.org/abs/1206.2944.
Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, and Jeff
Clune. Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural
Networks for Reinforcement Learning, April 2018. URL http://arxiv.org/abs/1712.06567.
arXiv:1712.06567 [cs].
Laurits Tani, Diana Rand, Christian Veelken, and Mario Kadastik. Evolutionary algorithms for hyperparameter
optimization in machine learning for application in high energy physics.The European Physical Journal C,
81(2):170, February 2021. ISSN 1434-6044, 1434-6052. doi: 10.1140/epjc/s10052-021-08950-y. URL
http://arxiv.org/abs/2011.04434. arXiv:2011.04434 [hep-ex].
Nico M Temme.Bessel Functions, chapter 9, pp. 219–255. John Wiley and Sons, Ltd, 1996. ISBN
9781118032572. doi: https://doi.org/10.1002/9781118032572.ch9. URL https://onlinelibrary.
wiley.com/doi/abs/10.1002/9781118032572.ch9.
16
Sebastian Towers, Aleksandra Kalisz, Philippe A. Robert, Alicia Higueruelo, Francesca Vianello, Ming-
Han Chloe Tsai, Harrison Steel, and Jakob N. Foerster. ADIOS: Antibody Development via Opponent
Shaping, June 2025. URLhttp://arxiv.org/abs/2409.10588. arXiv:2409.10588 [q-bio].
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.),Advances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_
files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
Paul Vicol, Luke Metz, and Jascha Sohl-Dickstein. Unbiased gradient estimation in unrolled computation
graphs with persistent evolution strategies. In Marina Meila and Tong Zhang (eds.),Proceedings of the
38th International Conference on Machine Learning, volume 139 ofProceedings of Machine Learning
Research, pp. 10553–10563. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/
v139/vicol21a.html.
Paul Vicol, Zico Kolter, and Kevin Swersky. Low-Variance Gradient Estimation in Unrolled Com-
putation Graphs with ES-Single, April 2023. URL http://arxiv.org/abs/2304.11153.
arXiv:2304.11153 [cs, stat].
Amala Mary Vincent and P. Jidesh. An improved hyperparameter optimization framework for AutoML
systems using evolutionary algorithms.Scientific Reports, 13(1):4737, March 2023. ISSN 2045-2322. doi:
10.1038/s41598-023-32027-3. URLhttps://doi.org/10.1038/s41598-023-32027-3.
G. N. Watson.A Treatise on the Theory of Bessel Functions. Cambridge University Press, Cambridge, 2
edition, 1944. Reprinted with corrections, various later printings.
G. B. Whitham.Linear and nonlinear waves. Pure and applied mathematics. Wiley-Interscience, New York,
1999. ISBN 9786613306241.
Daan Wierstra, Tom Schaul, Jan Peters, and Jürgen Schmidhuber. Natural evolution strategies. pp. 3381–3387,
06 2008. doi: 10.1109/CEC.2008.4631255.
Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, and Jürgen Schmidhuber. Natural evolution
strategies, 2011. URLhttps://arxiv.org/abs/1106.4487.
C.S. Withers. A simple expression for the multivariate hermite polynomials.Statistics and Probability Letters,
47(2):165–169, 2000. ISSN 0167-7152. doi: https://doi.org/10.1016/S0167-7152(99)00153-4. URL
https://www.sciencedirect.com/science/article/pii/S0167715299001534.
Ke Xue, Chao Qian, Ling Xu, and Xudong Fei. Evolutionary gradient descent for non-convex optimization.
In Zhi-Hua Zhou (ed.),Proceedings of the Thirtieth International Joint Conference on Artificial Intelli-
gence, IJCAI-21, pp. 3221–3227. International Joint Conferences on Artificial Intelligence Organization,
8 2021. doi: 10.24963/ijcai.2021/443. URL https://doi.org/10.24963/ijcai.2021/443.
Main Track.
Ziming Yu, Pan Zhou, Sike Wang, Jia Li, Mi Tian, and Hua Huang. Zeroth-order fine-tuning of llms in random
subspaces. InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp.
4475–4485, October 2025.
Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D.
Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, and Tianlong Chen. Revisiting zeroth-order
optimization for memory-efficient llm fine-tuning: A benchmark, 2024.
17
Appendix
A Proofs 19
A.1 ES Matrix Gradient Deviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.2 Error Rate Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B Mean Field Score Function Approximator 27
B.1 Bessel Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.2 Derivation of Mean-field Approximators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C EGG Architecture 33
C.1 Notation and Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
C.2 Parameter Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
C.3 Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.4 Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.5 Layer Normalization (LN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.6 MLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
C.7 GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
C.8 Fitness Calculation in Integer Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
D EGG Pretraining with Integer EGGROLL 35
D.1 Adding EGGROLL Perturbations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
D.2 Fitness Shaping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
D.3 Parameter Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
E EGG Ablations 36
E.1 Data Efficient Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
E.2 Controlling For Data Across Population Sizes . . . . . . . . . . . . . . . . . . . . . . . . . 37
F EGGROLL Speed 38
G Experimental Details 38
G.1 Reinforcement Learning Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
G.2 Multi Agent Reinforcement Learning Experiments . . . . . . . . . . . . . . . . . . . . . . 39
G.3 Reasoning Fine-tuning Experiments: Countdown . . . . . . . . . . . . . . . . . . . . . . . 41
G.4 Reasoning Fine-tuning Experiments: GSM8K . . . . . . . . . . . . . . . . . . . . . . . . . 43
18
A Proofs
In our proofs, we use the integral notation
∫
to denote the integral over the corresponding Rd space, for
example, for a matrix E∈R m×n,
∫
f(E)dE=
∫
Rm×n f(E)dE and for a vector E∈R mn,
∫
f(v)dv=∫
Rmn f(v)dv.
A.1 ES Matrix Gradient Deviations
Proposition 1.Using the Gaussian matrix policy π(M|µ) =N(µ,I mσ2,Inσ2) the ES objective can be
written as:
J(µ) =E E∼P(E) [f(M=µ+σE)].
and the matrix gradient of the ES objective is:
∇µJ(µ) =− 1
σEE∼P(E) [E·f(M=µ+σE)].
where P(E) is a zero-mean standard normal p(E) =N(0,I m,In). Moreover, the natural matrix gradient is
equal toσ2∇µJ(µ), i.e. equivalent to Eq.(4)up to a factor ofσ 2.
Proof. We start by deriving the derivative of the ES objective from Eq. (1). Taking the derivative with respect
toµ:
∇µJ(µ) =
∫
f(M)∇ µπ(M|µ)dM,
=
∫
f(M)∇ µlogπ(M|µ)π(M|µ)dM.
Now,
∇µlogπ(M|µ) =∇ µ − 1
2σ2 tr
(
(M−µ) ⊤(M−µ)
)
,
=− 1
σ2 (M−µ),
where we have used Petersen & Pedersen (2008, Eq. 103) for the matrix derivative of the trace, hence:
∇µJ(µ) =− 1
σ
∫ (M−µ)
σ f(M)π(M|µ)dM.
We make the transformation of variablesE= M−µ
σ :
∇µJ(µ) =− 1
σ
∫
E·f(M)p(E)dE,
where
p(E) =σmnπ(M=µ+σE|µ)
= 1
(2π)
mn
2
exp
(
−1
2tr
(
E⊤E
))
,
=N(0,Im,In),
as required. Using the same transformation of variables, the alternative form of the matrix Gaussian ES
objective follows immediately:
J(µ) =
∫
f(M)π(M|µ)dM,
=
∫
f(M=µ+σE)P(E)dE.
19
We now derive the natural policy gradient for a Gaussian policy to prove our final claim. It is convenient to work
in vector space rather than matrix space. In vector form, the natural gradient is defined as F(µv)−1∇µvJ(µv)
whereF(µ v)is the Fisher information matrix of the distribution:
F(µv) :=E v∼π(v|µv)
[
∇µv logπ(v|µv)∇µv logπ(v|µv)⊤]
,
whereµv =vec(M)andπ(v|µ v) =N(µ v,σ2Imn)for our Gaussian case. Deriving the score function:
∇µv logπ(v|µv) =− 1
2σ2 ∇µv∥µv −v∥2,
= 1
σ2 (v−µ v),
Hence the Fisher information matrix is:
F(µv) = 1
σ4 Ev∼π(v|µv)
[
(v−µ v)(v−µ v)⊤]
,
= 1
σ2 Imn.
This yields the natural gradient:
F(µv)−1∇µvJ(µv) =σ2∇µvJ(µv).
Converting back to matrix form, the natural gradient is thusσ2∇µJ(µ)as required.
Theorem 1.Let Assumption 1 hold. Then for finiter:
gLR :=∇µJLR(µ) =−1
σEZ∼p(Z) [∇Z logp(Z)f(y= (µ+σZ)x)].
where Z is generated by sampling A∼p(A),B∼p(B),ϵ∼p(ϵ) and making the transformation Z=
AB⊤
√r +ϵ.
Proof. We first prove that the density p(Z) of the pushforward distribution of p(A), p(B) and p(ϵ) under the
mapping (A,B,ϵ) :→Z= AB⊤
√r +ϵ exists. As ris finite, p(ϵ) is a Gaussian with well-defined density and Z
is a sum two independent variables AB⊤
√r and ϵ. The density p(Z) is thus given by the convolution (see, for
example, Durrett (2010, Theorem 2.1.11)):
p(Z) =E A∼p(A),B∼p(B)
[
p
(
ϵ=Z− AB⊤
√r
)]
,
which integrates to 1 over Rm×n and, since p(ϵ) is everywhere positive, is also everywhere positive. The
distribution of Zis thus absolutely continuous with respect to the Lebesgue measure in Rm×n and the density
p(Z)is well-defined. To derive the gradient, we make the transformation of variablesZ ′=µ+σZ:
JLR(µ) =
∫
p(Z′)f(y=Z ′x)dZ′,
wherep(Z′) = 1
σmnp(Z= Z′−µ
σ )Taking derivatives with respect toµ:
∇µJLR(µ) =
∫ 1
σmn∇µp
(
Z= Z′−µ
σ
)
f(y=Z ′x)dZ′,
=
∫
p(Z′)∇µlogp
(
Z= Z′−µ
σ
)
f(y=Z ′x)dZ′.
From the chain rule:
∇µlogp
(
Z= Z′−µ
σ
)
=−1
σ∇Z logp
(
Z= Z′−µ
σ
)
,
20
hence:
∇µJLR(µ) =−1
σ
∫
p(Z′)∇Z logp
(
Z= Z′−µ
σ
)
f(y=Z ′x)dZ′.
Making the reverse transformation of variablesZ= Z′−µ
σ :
∇µJLR(µ) =−1
σ
∫
p(Z)∇Z logp(Z)f(y= (µ+σZ)x)dZ,
=−1
σEZ∼p(Z) [∇Z logp(Z)f(y= (µ+σZ)x)],
as required.
A.2 Error Rate Analysis
For convenience, we work with random vectors in our analysis. Using the vec operator, we analyse the vector
vr =vec(Z r)where:
vec(M) = [m1,1,...m 1,m,m2,1,...m m,n]⊤.
For a vectorv∈R mn, we define the mat operator as:
mat(v) =


v1 vm+1 ... v (n−1)m+1
v2 vm+2 ... v (n−1)m+2
... ... ... ...
vm v2m ···v mn

,
so mat(vec(M)) =M . Using Eq. (6), write vr as a standardised sum of rindependent, zero-mean random
vectors. Let
ui =vec
(
aib⊤
i +ϵi
)
,(10)
where recall aiand biare the ith column vectors ofAand Band ϵiis an m×n random matrix with independent
Gaussian elements drawn fromN(0, σ2
ϵ/r), so:
vr = 1√r
r∑
i=1
ur
i.
We will use the fact that the Frobenius norm becomes theℓ-2 norm in vector space:
∥M∥F =
√∑
i,j
mi,j2 =
√∑
k
vec(M)k
2 =∥vec(M)∥.(11)
Denoting the covariance matrix of p(u) as Σr, the central limit theorem proves that the distribution of vr
converges in distribution to a zero-mean GaussianN(0,Σ r). In Lemma 1, derive the covariance matrix for Σr.
Our analysis uses an Edgeworth expansion (Bhattacharya & Ranga Rao, 1976) to characterise precisely the rate
at which p(vr) converges to the limiting Gaussian distribution. In Lemma 2, we make an Edgeworth expansion
of p(vr) to show that it is dominated by O
(1
r
)
terms and higher. These are then used to prove Lemma 3,
which allows us to bound the integral of the remainder of the Edgeworth expansion, thereby characterising
how fastp(vr)converges to the limiting Gaussian distribution.
Lemma 1.Let Assumption 1 hold and ube defined in Eq.(10). Then the variable uhas diagonal covariance
matrix:
Σr =
(
σ4
0 + σ2
ϵ
r
)
Imn =: σ2
rImn,
Moreover, the variableuhas a finite4th-order absolute moment:
Eu∼p(u)
[
∥u∥4]
<∞.
21
Proof.Under the vec operator, the vectorucan be written element wise as:
u= [a1b1 +ϵ1,1,a2b1 +ϵ2,1,...a mb1 +ϵm,1,a1b2 +ϵ1,2,...a mbn +ϵm,n]⊤.
We note that all elements in the vectoruhave zero mean, and so the covariance matrix is the expectation of the
outer product:
Σr =E u∼p(u)
[
uu⊤]
.
The diagonal elements ofΣ r are:
Eai,bj,ϵj,i
[
(aibj +ϵj,i)2]
=E ai,bj,ϵj,i
[
a2
ib2
j + 2aibjϵj,i + (ϵj,i)2
]
,
=E ai,bj
[
a2
ib2
j
]
+E ϵj,i
[
(ϵj,i)2
]
,
=σ4
0 + σ2
ϵ
r .(12)
As all elements ofa,bandϵare zero-mean, off-diagonal elements are zero:
Eai,bj,ak,bl,ϵj,i,ϵk,l [(aibj +ϵj,i) (akbl +ϵk,l)]
=Eai,bj,ak,bl [aibjakbl] +Eai,bj,ϵk,l [aibjϵk,l]
+E ak,bl,ϵj,i [ak,bl,ϵj,i] +Eϵj,i,ϵl,k [ϵj,iϵk,l],
=Eai,bj,ak,bl [aibjakbl] +Eϵj,i,ϵl,k [ϵj,iϵk,l],
=0i̸=korj̸=l.(13)
Using Eqs. (12) and (13), our first result follows:
Σr =
(
σ4
0 + σ2
ϵ
r
)
Imn.
Now, as uis a vector of elements which are sums and products of variables which all have finite 4th order
moments from Assumption 1, it immediately follows thatuhas finite4th order absolute moments.
Using Lemma 1, we see the asymptotic Gaussian density ofvr is:
g(vr) = 1√
(2π)mnσ2r
exp
(
−∥vr∥2
2σ2r
)
.(14)
We now derive a 4th order Edgeworth expansion forp(vr) and its score function, which expandsp(vr) in terms
of the limiting distribution g(vr) plus remainder terms. Our proof reveals that 3rd order cumulants control all
terms in the expansion that decay at rate O
(
1√r
)
. As 3rd order cumulants are all zero due to symmetry in
Assumption 1, the overall decay rate is controlled by O
(1
r
)
terms associated with 4th order cumulants. It is
for this reason that we obtain a faster convergence rate than the standard central limit theorem.
Lemma 2.Let Assumption 1 hold and let vr =vec(Z r) and ui be defined in Eq.(10). Let g(vr) denote
limiting Gaussian density in Eq.(14). Then, the Egdeworth expansion ofp(v r)is:
p(vr) =g(v r) + 1
4!rg(vr)
∑
i,j,k,l
κ4
i,j,k,lHi,j,k,l(vr,σr) +ρ(vr),,
where:
Hi,j,k,l(vr,σr) := exp
(∥vr∥2
2σ2r
) ∂4
∂vr
i∂vr
j∂vr
k∂vr
l
exp
(
−∥vr∥2
2σ2r
)
is a 4th order Hermite polynomial forg(vr)(Laplace, 1811; Hall, 1992; Withers, 2000) andρ(v r)iso
(1
r
)
.
22
Proof.We denote the characteristic function ofp(u r
i)as:
φUr(ω) =
∫
exp
(
−iω⊤u
)
p(u)du.
Now, make the transformation of variables xr
i = ur
i√r, hence p(xr
i) = √rp(ur
i = √rxr
i) and denote the
characteristic function of xr
i as φXr(ω). Using the time-scaling property of the Fourier transform, we can
derive the characteristic function ofxr
i in terms ofφUr(ω):
φXr(ω) =
∫
exp
(
−iω⊤x
)
p(x)dx,
= √r
∫
exp
(
−iω⊤x
)
p(u= √rx)dx,
= √r
∫ 1√rexp
(
−iω⊤ u√r
)
p(u)du,
=
∫
exp
(
−iω⊤ u√r
)
p(u)du,
=φUr
(ω√r
)
.(15)
Now, as vr = ∑r
i=1 xr
i, we use the property that the density of a sum of rindependent random variables
is given by the r-fold convolution of the individual PDFs. As convolution in the spatial domain is equal to
multiplication in the frequency domain, the densityp(vr)is:
p(vr) =F−1 [(φXr)r] (vr),
=F−1 [exp (rlogφXr)] (vr),
=F−1 [exp (rKr
X)] (vr),(16)
where Kr
X(ω) := logφXr(ω) is thecumulant generating function(also known as the second characteristic
function) forX r
i. From Eq. (15):
Kr
X(ω) = logφXr(ω),
= logφUr
(ω√r
)
,
=KUr
(ω√r
)
,
whereKUr (·)is the cumulant generating function foru r
i. We now make a 4th order Taylor series expansion
ofrKUr
(
ω√r
)
aboutω= 0:
rKUr
(ω√r
)
=rK Ur(0)
=log 1=0
+√r
∑
i
ωi
∂KUr(ω= 0)
∂ωi
+ 1
2!
∑
i,j
ωiωj
∂2KUr(ω= 0)
∂ωi∂ωj
+ 1
3!√r
∑
i,j,k
ωiωjωk
∂3KUr(ω= 0)
∂ωi∂ωj∂ωk
+ 1
4!r
∑
i,j,k,l
ωiωjωkωl
∂4KUr(ω= 0)
∂ωi∂ωj∂ωk
+o
(1
r
)
,
=√r
∑
i
ωiκ1
i + 1
2!
∑
i,j
ωiωjκ2
i,j + 1
3!√r
∑
i,j,k
ωiωjωkκ3
i,j,k
+ 1
4!r
∑
i,j,k,l
ωiωjωkωlκ4
i,j,k,l +o
(1
r
)
,
=√r
∑
i
ωiκ1
i + 1
2!
∑
i,j
ωiωjκ2
i,j + 1
3!√r
∑
i,j,k
ωiωjωkκ3
i,j,k +R(ω),(17)
23
whereκn
· is ann-th order cumulant (i.e.κ n
i := ∂KUr(ω=0)
∂ωi
) and
R(ω) := 1
4!r
∑
i,j,k,l
ωiωjωkωlκ4
i,j,k,l +o
(1
r
)
,
denotes the 4th order polynomial and o
(1
r
)
denotes the higher order remainder terms. We now state some facts
about cumulants to simplify the above expansion. For a symmetric random vector like ur under Assumption 3,
all odd-order cumulants are zero, i.e. κi
·= 0 for odd i(McCullagh, 2013; 1987). The second-order cumulant
term coincides with the covariance matrix product: ∑
i,jωiωjκ2
i,j =−ω⊤Σrω(McCullagh, 1987). Moreover,
as 4th order moments of ur exist from Lemma 1 then all cumulants up to including order 4 exist since
cumulants are polynomial functions of the moments (Leonov & Shiryaev, 1959; McCullagh, 1987). Finally,
we use ω⊤Σrω=∥ω∥ 2σ2
r from Lemma 1. Substituting these results into Eq. (17) and using R(ω) to denote
yields:
rKUr
(ω√r
)
=−∥ω∥2σ2
r
2 +R(ω),
=⇒exp (rK r
X(ω)) = exp
(
−∥ω∥2σ2
r
2 +R(ω)
)
,
= exp
(
−∥ω∥2σ2
r
2
)
exp (R(ω)),
= exp
(
−∥ω∥2σ2
r
2
)(
1 +R(ω) +o
(1
r
))
,
= exp
(
−∥ω∥2σ2
r
2
)
1 + 1
4!r
∑
i,j,k,l
ωiωjωkωlκ4
i,j,k,l +o
(1
r
)
,
where we have used the Taylor series expansion of exp(x) about x= 0 to derive the fourth line. Substituting
into Eq. (16) yields:
p(vr) =
∫
exp(iω⊤vr) exp (rKr
X(ω))dω,
=
∫
exp(iω⊤vr) exp
(
−∥ω∥2σ2
r
2
)
1 + 1
4!r
∑
i,j,k,l
ωiωjωkωlκ4
i,j,k,l +o
(1
r
)
dω,
=
∫
exp(iω⊤vr) exp
(
−∥ω∥2σ2
r
2
)
dω
+ 1
4!r
∑
i,j,k,l
κ4
i,j,k,l
∫
ωiωjωkωlexp(iω⊤vr) exp
(
−∥ω∥2σ2
r
2
)
dω+o
(1
r
)
,
=
∫
exp(iω⊤vr) exp
(
−∥ω∥2σ2
r
2
)
dω
+ 1
4!r
∑
i,j,k,l
κ4
i,j,k,l
∂4
∂vr
i∂vr
j∂vr
k∂vr
l
∫
exp(iω⊤vr) exp
(
−∥ω∥2σ2
r
2
)
dω+o
(1
r
)
.
Now, recognising thatexp
(
−∥ω∥2σ2
r
2
)
is the characteristic function forg(vr)yields our desired result:
p(vr) =F−1[F[g]](vr) + 1
4!r
∑
i,j,k,l
κ4
i,j,k,l
∂4
∂vr
i∂vr
j∂vr
k∂vr
l
F−1[F[g]](vr) +o
(1
r
)
,
=g(v r) + 1
4!r
∑
i,j,k,l
κ4
i,j,k,l
∂4
∂vr
i∂vr
j∂vr
k∂vr
l
g(vr) +o
(1
r
)
,
=g(v r) + 1
4!rg(vr)
∑
i,j,k,l
κ4
i,j,k,lHi,j,k,l(vr,σr) +ρ(vr),
24
whereρ(vr)denotes the remainder terms.
We now provide uniform bounds on the remainder term of the expansion using the techniques of (Bhattacharya
& Ranga Rao, 1976, Theorem 19.2) and analyse convergence rate of the limiting Gaussian distribution to a
unit Gaussian:
Lemma 3.Let σ02 = 1, let g(vr) be the limiting Gaussian from Eq.(14), let f(vr) be a bounded function and
let g∞(vr) =N(0,I mn). Let Assumptions 1 and 2 hold and let vr =vec(Z r) and ui be defined in Eq.(10).
Define the Edgeworth remainder as:
R(vr) =p(vr)−g(v r).
Then: ∫
f(vr)vrR(vr)dvr =O
(1
r
)
,
∫
vrf(vr)g(vr)dvr =
∫
vrf(vr)g∞(vr)dvr +O
(1
r
)
.
Proof. We start by bounding
∫
f(vr)vrR(vr)dvr. From Lemma 2, we have shown that the Edgeworth
expansion forp(vr)is controlled by 4th order cumulants and higher, that is;
p(vr) =g(v r) + 1
4!rg(vr)
∑
i,j,k,l
κ4
i,j,k,lHi,j,k,l(vr,σr) +ρ(vr).(18)
We show that the three assumptions needed to apply Bhattacharya & Ranga Rao (1976, Theorem 20.1) to
obtain our result using Eq. (18). Firstly, the boundedness assumption of the integrand holds:
sup
vr
∥f(vr)vr∥
1+∥vr∥ ≤sup
vr
f(vr)<∞.
Secondly, the sampling regularity assumption that ui (as defined in Eq. (10)) is zero-mean i.i.d. (satisfied
under Assumption 1) with finite 4th order moments (satisfied from Lemma 1) holds. Let φU(ω) denote
the characteristic function of p(u), then the final assumption we need to verify is Cramer’s condition:
lim sup∥ω∥→∞φU(ω)<1 , which is satisfied from the Riemann-Lebesgue lemma Folland (1999, Theo-
rem 8.22) because p(u) is absolutely continuous under Assumption 1). Our first result thus follows from
applying Bhattacharya & Ranga Rao (1976, Theorem 20.1):
∫
f(vr)vrR(vr)dvr =O
(1
r
)
.
We now derive our second result. Letd := mn
2 . Asg∞(vr)andg(v r)are both Gaussians:
g∞(vr)−1g(vr) =
(1
σr
)d
exp
(∥vr∥2
2 −∥vr∥2
2σr2
)
,
=
( r
r+σ ϵ2
)d
exp
(∥vr∥2
2 − r∥vr∥2
2(r+σ 2ϵ)
)
,
=
( r
r+σ ϵ2
)d
exp
(σ2
ϵ∥vr∥2
2(r+σ 2ϵ)
)
,
=
( r
r+σ ϵ2
)d(
1 + exp
(σ2
ϵ∥vr∥2
2(r+σ 2ϵ)
)
−1
)
,
where we have substituted forσr2 = 1 + σϵ
2
r from Lemma 1, hence:
∫
vrf(vr)g(vr)dvr =
∫
vrf(vr)g∞(vr)g∞(vr)−1g(vr)dvr,
=
∫
vrf(vr)g∞(vr)
( r
r+σ ϵ2
)d(
1 + exp
(σ2
ϵ∥vr∥2
2(r+σ 2ϵ)
)
−1
)
dvr.
25
Expanding
(
r
r+σϵ2
)d
using the binomial expansion:
( r
r+σ ϵ2
)d
=
(
1− σϵ2
r+σ ϵ2
)d
,
= 1− dσϵ2
r+σ ϵ2 +o
( dσϵ2
r+σ ϵ2
)
,
= 1 +O
(1
r
)
,
hence:
∫
vf(vr)g(vr)dvr =
∫
vrf(vr)g∞(vr)g∞(vr)−1g(vr)dvr,
=
(
1 +O
(1
r
))∫
vrf(vr)g∞(vr)
(
1 + exp
(σ2
ϵ∥vr∥2
2(r+σ 2ϵ)
)
−1
)
dvr.
It therefore suffices to show:
∫
vrf(vr)g(vr)
(
exp
(σ2
ϵ∥vr∥2
2(r+σ 2ϵ)
)
−1
)
dvr =O
(1
r
)
.
Using the identityexp(x)−1≤xexp(x)forx≥0:

∫
vrf(vr)g(vr)
(
exp
(σ2
ϵ∥vr∥2
2(r+σ 2ϵ)
)
−1
)
dvr

≤
∫
|f(vr)|∥v∥g∞(vr) σ2
ϵ∥vr∥2
2(r+σ 2ϵ) exp
(σ2
ϵ∥vr∥2
2(r+σ 2ϵ)
)
dvr
=
∫
g∞(vr)σ2
ϵ|f(vr)|∥vr∥3
2(r+σ 2ϵ) exp
(σ2
ϵ∥vr∥2
2(r+σ 2ϵ)
)
dvr
=
∫ 1
(2π)d
σ2
ϵ|f(vr)|∥vr∥3
2(r+σ 2ϵ) exp
(
−∥vr∥2
2 + σ2
ϵ∥vr∥2
2(r+σ 2ϵ)
)
,
≤sup
v
|f(vr)|(2π)dσ2
ϵ
2(r+σ 2ϵ)
∫
∥vr∥3 exp
(
−∥vr∥2
2
( r
r+σ 2ϵ
))
dvr
  
=O(1)
,
=O
(1
r
)
,
where our final line follows from the fact that fixed polynomials are integrable under the Gaussian distribution
andf(v r)is bounded by assumption, hence:
∫
vf(vr)g(vr)dvr =
(
1 +O
(1
r
))(∫
vrf(vr)g∞(vr)dvr +O
(1
r
))
,
=
∫
vrf(vr)g∞(vr)dvr +O
(1
r
)
,
as required.
Using Lemma 3, we have all ingredients needed derive our main about the convergence result, which follows
after some algebra on the norm:
Theorem 2.Let Assumptions 1 and 2 hold and setσ 0 = 1, then:
∥gTrue −ˆgr
LR∥F =O
(1
r
)
.
26
Proof.We start by converting the norm to vector form using Eq. (11):
σ(gTrue −gr
LR) =E E[E·f(M=µ+σE)]−E Zr [Zrf(M=µ+σZ)],
=E v[vf(µ,v)]−E vr [vrf(µ,vr)],(19)
where f(µ,v) :=f(M=µ+σmat(v)) and v=vec(E) is the vectorisation of variableE, which is distributed
as v∼g ∞(v) =: N(0,Imn). Let R(vr) denote the difference between p(vr) and the limiting Gaussian
distributiong(vr) := 1√
(2π)mnσ2r
exp
(
−∥vr∥2
2σ2r
)
:
R(vr) :=p(vr)−g(v r).
Using this notation:
Evr [vrf(µ,vr)] =
∫
p(vr)vrf(µ,vr)dvr,
=
∫
(g(vr) +R(vr))vrf(µ,vr)dvr,
=
∫
g(vr)vrf(µ,vr)dvr +
∫
R(vr)vrf(µ,vr)dvr,
where g(vr) is the limiting Gaussian density from Eq. (14) and R(vr) is the Edgeworth remainder term as
defined in Lemma 2. Now, applying Lemma 3 to both integrals:
Evr [vrf(µ,vr)] =
∫
g∞(vr)vrf(µ,vr)dvr +O
(1
r
)
,
=Ev[vf(µ,v)] +O
(1
r
)
.
Substituting into Eq. (19) yields our desired result:
σ(gTrue −gr
LR) =O
(1
r
)
,
=⇒ ∥gTrue −ˆgr
LR∥F =O
(1
r
)
.
B Mean Field Score Function Approximator
B.1 Bessel Functions
We will use nth order Bessel functions of the second kind Kn(z) (Basset, 1892; Macdonald, 1899; Watson,
1944), which are conveniently represented by the integral equations:
Kn(z) =
∫∞
0
exp(−zcoshθ) cosh(nθ)dθ.
Bessel functions are the solutions to systems of differential equations that occur naturally in phenomena
where there is strong radial symmetry, typically involving the propagation of spherical waves from points
like the ripples formed from water droplets (Whitham, 1999). For our setting, Bessel functions describe the
probability density of the product of rotationally invariant random variables, whose solution is analogous to
the interference pattern of two spherical wave propagators.
Using the representation, we find the derivative of the zeroth order function takes the recursive form:
dK0(z)
dz =−
∫∞
0
exp(−zcoshθ) cosh(θ)dθ=−K 1(z).(20)
More generally, the derivative of thenth order Bessel function is Watson (1944, Section 3.71, Eq. 4):
dKn(z)
dz = n
zKn(z)−K n+1(z).(21)
27
B.2 Derivation of Mean-field Approximators
To derive a mena-field approximation, we assume that the elements of Aand Bare drawn independently from
the set of generalised Gaussian distributions (GGDs):
Assumption 3.Assume each element ai,j ∼GG(s,p) and bi,j ∼GG(s,p) of Aand B be independently
distributed according to the zero-mean generalised Gaussian distributionGG(s,p)with density:
GG(x|s,p) = p
2sΓ
(
1
p
)exp
(
−
⏐⏐⏐x
s
⏐⏐⏐
p)
,
where0<s<∞is the scale parameter,p>0the shape parameter andΓ(·)is the gamma function.
We observe common distributions emerge from the set of GGDs including the Laplace forp= 1 , the Gaussian
forp= 2and the uniform over[−s,+s]in the limitp→∞.
If we make the assumption that all elements of Zare independent (this is true as rgrows) then we can write
p(Zr)≈ˆp(Z) := ∏m
i=1
∏n
j=1 p(zi,j) as the product of the marginal distributions. Under this approximation,
the score function can be defined element-wise as:
[∇Z logp(Zr)]i,j ≈ˆS(zi,j) :=∂zi,j logp(zi,j).
Using this approximation we apply the score function ˆS(·)element-wise to the matrixZ:
gLR ≈ˆgMF :=−σE Z∼p(Zr)
[
f(y= (µ+σZ)x)· ˆS⊙(Z)
]
.
Forr= 1, ˆS(·)has a convenient analytic form for all members of the set of GGDs withσ ϵ ≈0:
Theorem 3.Let Assumption 3 hold withσ ϵ = 0andr= 1. Then the distribution over marginalsp(z i,j)is:
p(zi,j) = p
(
sΓ
(
1
p
))2 K0
(
2|zi,j|
p
2
sp
)
,(22)
where K0 (·) is the zeroth-order modified Bessel function of the second kind and the marginal score function is
defined element-wise as:
ˆS(zi,j) =−
K1
(
2|zi,j|
p
2
sp
)
K0
(
2|zi,j|
p
2
sp
)·p
p
2 |zi,j|
p
2 −1sign(zi,j)
sp .
Proof. For r= 1 , we denote the elements of vector Aas ai and elements of vector Bas bj, then the elements
of matrix Z=AB ⊤are: zi,j =aibj. We now derive the distribution the unnormalised variables: zi,j using
the formula for the distribution of the product of two independent random variables (Rohatgi, 1976; Grimmett
& Stirzaker, 1993):
p(zi,j) =
∫∞
−∞
p(ai)p
(
bj = zi,j
ai
) 1
|ai|dai,
=

 p
2sΓ
(
1
p
)


2 ∫∞
−∞
exp
(
−
⏐⏐⏐ai
s
⏐⏐⏐
p)
exp
(
−
⏐⏐⏐⏐
zi,j
ais
⏐⏐⏐⏐
p) 1
|ai|dai,
=2

 p
2sΓ
(
1
p
)


2 ∫∞
0
exp
(
−
⏐⏐⏐ai
s
⏐⏐⏐
p)
exp
(
−
⏐⏐⏐⏐
zi,j
ais
⏐⏐⏐⏐
p) 1
|ai|dai,
28
where we have used symmetry of the integrand about 0 to derive the final line. Now, making the substitution
x=
(ai
s
)p
, we have:
dai
dx = sx
1
p−1
p , a i =sx
1
p
hence:
p(zi,j) = p
(
sΓ
(
1
p
))2
1
2
∫∞
0
exp
(
−x− 1
x
|zi,j|p
s2p
)1
xdx.
Now, we use the identity (Temme, 1996, Theorem 9.42):
K0(x) = 1
2
∫∞
0
exp
(
−x− z2
4x
)1
xdx,
withz= 2|zi,j|
p
2
sp to yield:
p(zi,j) = p
(
sΓ
(
1
p
))2 K0
(
2|zi,j|
p
2
sp
)
,
as required for Eq. (22). Now we derive the marginal score function by applying the chain rule:
∂zi,j logp(zi,j) =∂zi,j logK0
(
2|zi,j|
p
2
sp
)
,
=∂zlogK0
(
z= 2|zi,j|
p
2
sp
)
∂zi,j
2|zi,j|
p
2
sp ,
=∂zlogK0
(
z= 2|zi,j|
p
2
sp
)
p|zi,j|
p
2 −1sign(zi,j)
sp ,
=
∂zK0
(
z= 2|zi,j|
p
2
sp
)
K0
(
z= 2|zi,j|
p
2
sp
) ·p|zi,j|
p
2 −1sign(zi,j)
sp ,
=−
K1
(
2|zi,j|
p
2
sp
)
K0
(
2|zi,j|
p
2
sp
)·p|zi,j|
p
2 −1sign(zi,j)
sp ,
where we have used the identity∂zK0(x) =−K1(x)from Eq. (20)
Forr>1we can derive ˆS(·)for the Gaussian sampling case:
Theorem 4.Let Assumption 3 hold withσ ϵ = 0andp= 2. Then the distribution over marginalsp(z i,j)is:
p(zi,j) = 2√r|√rzi,j|
r−1
2
sr+1√πΓ
(r
2
) ·K r−1
2
(2|√rzi,j|
s2
)
.
and the score function is:
ˆS(zi,j) = r−1
zi,j
−2√rsign(zi,j)
s2
Kr+1
2
(
2|√rzi,j|
s2
)
Kr−1
2
(
2|√rzi,j|
s2
),.
29
Proof. Let ui,j = 1√rzi,j. Using the transformation of variables, we derive the density for zi,j from p(ui,j)
using: p(zi,j) = √rp(ui,j = √rzi,j). Each element ui,j is the sum ofrindependent variables ui,j,l :=ai,lbj,l
distributed according to Eq. (22) withp= 2:
ui,j =
r∑
l=1
ai,lbj,l = 1√r
r∑
l=1
ui,j,l
From Eq. (22), the distribution of eachui,j,l is:
p(ui,j,l) = 2
s2πK0
(2|ui,j,l|
s2
)
We use the fact that the PDF of a sum of rindependent random variables is given by the r-fold convolution of
the individual PDFs. As convolution in the spatial domain is equal to multiplication in the frequency domain,
the PDF p(ui,j) follows by taking Fourier transform ofp(ui,j,l), taking the power rand then taking the inverse
Fourier transform:
p(ui,j) =
( 2
s2π
)r
F−1
[
F
[
K0
(2|·|
s2
)]r]
(ui,j),
where F[f(·)](ω) and F−1[ ˆf(·)](x) denotes the Fourier and inverse Fourier transforms. Taking the Fourier
transform of the Bessel function:
F
[
K0
(2|·|
s2
)]
(ω) =
∫
exp(−iω⊤x)K0
(2|x|
s2
)
dx,
=
∫
cos(ωx)K0
(2|x|
s2
)
dx−i
∫
sin(ωx)K0
(2|x|
s2
)
dx,
=
∫
cos(ωx)K0
(2|x|
s2
)
dx,
= 2
∫∞
0
cos(ωx)K0
(2x
s2
)
dx,(23)
where we have used the fact that K0
(
2|x|
s2
)
is an even function of xand so its integral with sin(ωx) in the
second line is zero. Using a standard result, we can evaluate the integral in Eq. (23) Gradshte˘ın et al. (2015,
6.671 Integral 14):
F
[
K0
(2|·|
s2
)]
(ω) = π√
ω2 +
(2
s2
)2
,
30
hence:
p(ui,j) =
( 2
s2π
)r
F−1

 πr
(
ω2 +
(2
s2
)2)r
2

(ui,j),
=
(2
s2
)r
F−1


(
ω2 +
(2
s2
)2)−r
2

(ui,j),
=
(2
s2
)r 1
2π
∫
exp(iωui,j)
(
ω2 +
(2
s2
)2)−r
2
dω,
=
(2
s2
)r 1
2π
(∫
cos(ωui,j)
(
ω2 +
(2
s2
)2)−r
2
dω
+i
∫
sin(ωui,j)
(
ω2 +
(2
s2
)2)−r
2
dω
)
,
=
(2
s2
)r 1
2π
∫
cos(ωui,j)
(
ω2 +
(2
s2
)2)−r
2
dω,
=
(2
s2
)r 1
π
∫∞
0
cos(ωui,j)
(
ω2 +
(2
s2
)2)−r
2
dω,(24)
where we have used the fact that the integrand is an even function and so its integral withsin(ωzi,j) is zero
to derive the penultimate line. To evaluate the integral in Eq. (24) we apply Gradshte ˘ın et al. (2015, 3.771
Integral 2):
p(ui,j) =
(2
s2
)r
· 1√πΓ
(r
2
)
(s2|ui,j|
4
)r−1
2
·K r−1
2
(2|ui,j|
s2
)
,
= 2|ui,j|
r−1
2
sr+1√πΓ
(r
2
)·K r−1
2
(2|ui,j|
s2
)
.
Using the transformation of variables yields our desired results:
p(zi,j) = √rp(ui,j = √rzi,j),
= 2√r|√rzi,j|
r−1
2
sr+1√πΓ
(r
2
) ·K r−1
2
(2|√rzi,j|
s2
)
.
Now, we derive the score function:
∂zi,j logp(zi,j) = r−1
2 ·∂zi,j log|√rzi,j|+∂ zi,j logKr−1
2
(2|√rzi,j|
s2
)
,
= r−1
2zi,j
+ 2∂zi,j|√rzi,j|
s2
∂xKr−1
2
(
x= 2|√rzi,j|
s2
)
Kr−1
2
(
2|√rzi,j|
s2
) ,
= r−1
2zi,j
+ 2√rsign(zi,j)
s2
∂xKr−1
2
(
x= 2|√rzi,j|
s2
)
Kr−1
2
(
2|√rzi,j|
s2
) ,
31
Now, from Eq. (21):
∂xKr−1
2
(x)
Kr−1
2
(x) =
r−1
2x Kr−1
2
(x)−K r+1
2
(x)
Kr−1
2
(x) ,
= r−1
2x −
Kr+1
2
(x)
Kr−1
2
(x),
=⇒∂ zi,j logp(zi,j) = r−1
2zi,j
+ (r−1)sign(z i,j)
2|zi,j| −2√rsign(zi,j)
s2
Kr+1
2
(
2|√rzi,j|
s2
)
Kr−1
2
(
2|√rzi,j|
s2
),
= r−1
2zi,j
+ (r−1)
2zi,j
−2√rsign(zi,j)
s2
Kr+1
2
(
2|√rzi,j|
s2
)
Kr−1
2
(
2|√rzi,j|
s2
),
= r−1
zi,j
−2√rsign(zi,j)
s2
Kr+1
2
(
2|√rzi,j|
s2
)
Kr−1
2
(
2|√rzi,j|
s2
),
as required.
32
C EGG Architecture
In the following section, we detail the design of our EGG model, which follows the high-level structure of
modern pre-layernorm decoder-only language models, but replaces self-attention with a modified minGRU and
standard layernorms with a custom variant to enable pure integer training. See Algorithm 2 for an overview of
the forward pass of the EGG architecture.
Algorithm 2EGG forward pass
Require:Input tokent∈U 8, input states∈I l×D
8 , network parametersθ
Ensure:Output vectory∈I D
8 and output states′∈I l×D
8
s′←I l×D
8 initialized to 0
y←EMBED(θ emb,t)
fori∈{0,...,l−1}do
y′,s′
i ←GRU(θgru,i,LN(θln1,i,y),s i)
y←I 8(I32(y′) +I32(y))
y′←MLP(θmlp,i,LN(θln2,i,y))
y←I 8(I32(y′) +I32(y))
end for
y←LN(θ lnout,i,y)@θT
head
C.1 Notation and Operations
We use the constant l∈E + to denote the number of layers of the model and D= 4 d as the hidden dimension
of the model, whered∈E +.
We use In to denote an n-bit signed integer and Un to denote an n-bit unsigned integer. We denote casting
vector ⃗ xto format In as In(⃗ x), which implicitly includes clipping to the bounds of the datatype. To ensure
symmetry between positive and negative values of each datatype, we consider the value−2n−1 to be invalid
for datatypeI n; for instance, for 8-bit signed integers we only allows value from -127 to 127.
We use the following operations:
•⃗ x@Mindicating scaled vector-matrix multiplication ofIn
8 ×In,m
8 →I m
8 , corresponding to int8 tensor
core multiplication with int32 accumulation and scaling. The details of this operation are described
in the next subsection.
•a·b indicates dot product with int32 accumulation, In
8 ×I n
8 →I 32, and a⊙b indicates the Hadamard
(elementwise) product.
• Standard integer operations: + for addition, −for subtraction, and ⊙for element-wise multiplication.
•|x|indicates taking the element-wise absolute value ofx,I n →I n.
• sign(x) indicates taking the element-wise sign of x, giving 1 for positive values, -1 for negative
values, and 0 for zero.
• sum(x)indicates taking the sum of all elements inx(casting toI 32 to prevent overflow):I n →I 32.
•x≫n indicates an elementwise bitwise right shift by n, which is typically equivalent to 2−nx.
Similarly,x≪nindicates a bitwise left shift byn, which is typically equivalent to2 nx.
• Square-bracket indexing. For instance M[x,y] extracts the element at index xin axis 0 and index y
in axis 1, following the zero-based indexing convention.
C.2 Parameter Initialization
The standard initialization for matrix parameters in our model is rounding 16 times a sample from the standard
normal, and casting to I8. This can be precomputed on a CPU since this is only done once at the start of
training.
33
The egg model has the following parameters (where an additional subscript of iindicates that there is a version
of this parameter for each layer of the model):
•θ emb ∈I 256×D
8 , following standard initialization.
•θ head ∈I 256×D
8 , following standard initialization.
•θ lnout ∈I D
8 , initialized to 16 for each element.
•θ ln1,i,θln2,i ∈I D
8 , initialized to 16 for each element
•θ mlp,i,1 ∈I 4D×D
8 andθmlp,i,2 ∈I D×4D
8 , following standard initialization.
•θ GRU,i,[Wf,Uf,Wh,Uh] ∈I D×D
8 , following standard initialization.
•θ GRU,i,[bfm bh] ∈I D
8 , initialized to 0 for each element.
In total there are513D+l(4D+ 12D 2)parameters in the model.
C.3 Matrix Multiplication
Tensor cores in GPUs are able to calculate fast vector-matrix multiplications with int32 accumulation as
xM∈I m
32 where x∈I n
8 and M∈I n×m
8 . For our purposes, we define x@M as a scaled multiplication:
x@M:=I 8( xM
16√n).
Note that when n= 4 d, the division operation just becomes a right-shift by 4 +d, which is fast to calcu-
late.
We choose this scaled matrix multiplication because we initialize M to 16 times standard normal samples for
each element, so dividing by 16√npreserves the magnitude of xfor the output. In particular, if all elements
of xand M are drawn from independently from the standard normal distribution multiplied by 16, the central
limit theorem tells us that the expected value per element of the output will be 256√n, so dividing by 16√n
preserves the standard deviation of 16.
C.4 Embedding
Our embedding function takes as input an embedding matrix θemb ∈I 256×D
8 and an input token t∈U 8, and
simply outputs the vector corresponding to that token:θemb[t]∈I D
8 .
C.5 Layer Normalization (LN)
Our layer normalization operation involves multiplying our input x∈I D
8 with a weight θln ∈I D
8 before
dividing by the mean absolute value ofx.
We decide to divide by the mean absolute value of the input instead of the more more common root-mean-
squared since square roots are expensive on integers. Note that the L1 norm after dividing the input by the
mean absolute value (when using real numbers) isDinstead of 1, which we intentionally choose to preserve
more bits of information given the limited range ofI 8.
We calculate the mean absolute value of inputxas:
xmav =I 8(sum(|x|)≫(2d)),
Note that we can safely cast the mean absolute value to an I8 without overflow given the properties of the
mean of a set, though we lose precision due to truncating the fractional component.
The output of layernorm is calculated as:
DIVIDE(I16(x)⊙I 16(θln),xmav).
Since division is an expensive operation, we precompute it using a lookup table. Note that the product of two
I8 values will always remain in the dynamic range of I16, so our lookup table will be of shape 216 ×2 8.
34
C.6 MLP
Each MLP block consists of two weight parameters: θ1 ∈I 4D×D
8 and θ2 ∈I D×4D
8 . Given an input x∈I D
8 ,
we calculate the output as:
(x@θT
1 )@θT
2 .
Note that we do not use an activation function, because the@ operation is already nonlinear due to the saturated
conversion fromI 32 toI 8
C.7 GRU
Each GRU block accepts an input vector and state x,s∈I D
8 consists of 6 weight parameters:
θWf,θUf,θWh,θUh ∈I D×D
8 andθbf,θbh ∈I D
8 .
Using these weight matrices, we calculate the following vectors:
f=σ(I 8(I32(x@θT
Wf) +I32(s@θT
Uf) +I32(θbf))),
ˆf=I 8(((I32(f) + 127)⊙I 32(s))≫8),
ˆh=ϕ(I 8(I32(x@θT
Wh) +I32( ˆf@θT
Uh) +I32(θbh))),
h=s+I 8(((I32(f) + 127)⊙(I 32(ˆh)−I 32(s)))≫8),
where his the output and the new hidden state. In the typical GRU, σstands for the sigmoid function while ϕ
stands for the hyperbolic tangent, but we find that setting these as identity operations is sufficient due to the
nonlinearity already present in the clipped addition. One can view this clipped addition operation as scaled
and shifted version of the “hard" tanh and sigmoid operators.
To explain why we perform these operations, we can analyze this relative to the original GRU. Thef vector
for the standard GRU has all elements between 0 and 1 due to the sigmoid, but our elements are between -127
and 127. Therefore, to calculate ˆf (which is typically just f⊙s ), we first add 127 to f, getting the range
between 0 and 254 before multiplying by sbefore bit-shifting right by 8 again to bring our values back to the
I8 dynamic range. We apply similar logic to calculate the final h, which is typically just h=s+f⊙( ˆh−s)
but needs to be rescaled to keep the int8 dynamic range.
C.8 Fitness Calculation in Integer Types
The “fitness” used in language model pretraining is the log-likelihood of correctly generating the next token,
treating the outputs of the language model as logits (unnormalized log probabilities). If t′∈U 8 is the next
token to predict andy∈I 256
8 are the logits, we can calculate the log likelihood as follows:
y′=I 32(y) + 128,
o=y ′[t′]−LOG2[sum(EXP2[y ′])],
whereois the loss for one token. We implement EXP2 and LOG2 as lookup tables, where
EXP2[i] = 16×2 i/16,
LOG2[i] = 16×log 2(i/16).
Note that each element in EXP2 for any U8 input requires at most 20 bits, so the sum of exponents across all
possible choices is at most 28 bits, meaning we have to precompute LOG2 for2 28 values.
D EGG Pretraining with Integer EGGROLL
The core ideas of EGGROLL still apply in this integer-based training setting, but we have to make some
modifications to ensure it only uses integer operations.
35
D.1 Adding EGGROLL Perturbations
For parameter θ∈I m×n
8 that represents a matrix multiplication, we first sample rank-1 perturbation vectors
for each index in the batch: A∈I m
8 and B∈I n
8 . We sample these vectors from the standard random normal
multiplied by 16 and rounded to the nearest I8 (clipping if necessary). To prevent the use of floating-point
arithmetic on the accelerator, we pre-generate a large matrix of these random values, randomly indexing into it
to get the perturbation vectors.
Given an inputx∈I n
8 , instead of calculatingx@θT, we calculate
I8(xθT + ((x·B)I 32(A)≫(4 + ˆσ))
16√n ).
The value of ˆσis a hyperparameter, related to the σin the main paper as σ= 2 −ˆσ. Note that the batched
forward pass remains efficient since it still simply performs a batched vector-vector dot product in int8 (with
int32 accumulate) and a batched vector-scalar product in int32.
We apply this same logic to the embedding matrix, since we can interpret θ[t] as one_hot(t)θand still apply
our rank-1 updates in that context. In practice, this means replacingx·BwithB[t].
D.2 Fitness Shaping
We employ a simple fitness shaping scheme based on antithetical pairs. Specifically, given raw fitnesses
s+,s−, for the positive and negative sample of the antithetical pair respectively, the transformed fitness for the
noise is:
sign(s+ −s−),
Note that the only possible values for the fitness after shaping are{−1,0,1}.
D.3 Parameter Update
For parameter θ∈I m×n
8 that represents a matrix multiplication (or embedding vector), suppose the sampled
batch of rank-1 perturbation vectors are A∈I N×m
8 and B∈I N×n
8 , and let the fitnesses after shaping be
F∈I N
8 . Then we calculate an intermediate valueE∈I m×n
32 as:
E= (F⊙A) TB.
We use Eto determine if each element of θshould be increased or decreased. In particular, when the absolute
value of E is above a pre-specified threshold we move θby one discrete bin in the direction of the sign of
E. Since there are only 255 unique values for each element in I8, restricting updates to single bins improves
stability without compromising the ability for a parameter to get to any other value with relatively few
updates.
We currently do not incorporate any momentum or other optimizer states, but this remains critical future work
to improve the speed of convergence for pure integer training.
E EGG Ablations
E.1 Data Efficient Training
In our main experiments, we give a unique training sequence to each antithetical pair. However, after fitness
shaping only 1 bit is extracted per antithetical pair, so we may be able to reuse training sequences across
multiple members of the population. When we let each sequence be used for at most 512 members of the
population, we get the loss curves in Fig. 6. At large population sizes, at 4096 and above, we see similar
performance despite using 256 times less data. At small population sizes, we see instability, likely due to the
fact that reusing the same training sequence across the entire population would lead to “overfitting” to the
training sequence instead of generalizing.
36
0 200 400 600 800 1000
Training Step
4
5
6
7
8Loss (bits/byte)
Data Efficient Pure Integer Pretraining: Test Loss
101
102
103
104
105
Population Size
Figure 6: Test loss curves when reusing each sequence across 512 members of the population. Dashed lines indicate the
original experiments where each antithetical pair receives a different sequence.
E.2 Controlling For Data Across Population Sizes
To determine whether the benefits of large population sizes are due to receiving more data or more unique
perturbations, we run an experiment where smaller population sizes receive the same amount of data per step
as the default for 32768. For instance, for a population size of 2, this means there is just one antithetical
pair of perturbations but each is evaluated across 16384 data sequences. In principle, this means that each
perturbation could have a cleaner fitness estimate. We plot results for population sizes of 2, 64, 512, 4096, and
32768 in Fig. 7.
0 200 400 600 800 1000
Training Step
4
5
6
7
8Loss (bits/byte)
Data Controlled Pure Integer Pretraining: Test Loss
10−1
100
101
102
103
104
105
Population Size
Figure 7: Test losses when different population sizes receive the same amount of data.
We find that maximizing the number of unique perturbations is critical for pretraining performance. Training
is incredibly unstable at a population size of 2, which is analogous to zeroth order optimization, implying that
pretraining would have been infeasible with prior zeroth order optimization approaches for LLM training or
other ES works. In particular, Qiu et al. (2025) manages to get satisfactory speed with naïve ES in reasoning
tasks by reusing the same perturbation hundreds of times and having a small population size, but Fig. 7
demonstrates that this strategy would not work for pretraining from scratch.
37
F EGGROLL Speed
All timings were done on a single GPU on a GH200 (equivalent to a single H100) for a linear model with
dimension 8192 in bfloat16, allowing a maximum batch size of 1024. For the graph in Fig. 2a, we pre-generate
the noises instead of integrating the noise generation into the forward pass.
EGGROLL PPO OpenES
0
20
40
60
80
100Normalized Speed
91 (69)
34
0.41 (0.054)
Normalized Training Speeds
Figure 8: Relative speed of EGGROLL, when including jax noise regeneration.
In Fig. 8, we consider the impact of regenerating noises on-the-fly using jax PRNG. The darker area and value
in parenthesis for EGGROLL and OpenES indicate the speed when regenerating noises on-the-fly, while the
full bar indicates the speed when the noises are already generated.
We regenerate noises on the fly in our primary jax codebase, but pre-generating the EGGROLL perturbations
beforehand is also a practical possibility since low-rank perturbations only require a small amount of memory,
proportional to the square root of the size of the original parameter matrices.
G Experimental Details
G.1 Reinforcement Learning Experiments
We present here the hyperparameter ranges we used for hyperparameter optimization, as well as all hyperpa-
rameter settings for all the experiments. All RL experiments were run on an NVIDIA L40S GPU. For PPO,
we use the same methodology to tune the hyperparameters as we did for OpenES and EGGROLL as described
in Section 6.2. We report the ranges and the final hyperparameters here. We train PPO agents using Rejax
(Liesen et al., 2024).
38
0 2 4
Steps 1e8
0.0
0.2
0.4
0.6
0.8
1.0Normalized Return
CartPole-v1
EggRoll
OpenES
PPO
0 2 4
Steps 1e8
0.0
0.2
0.4
0.6
0.8
1.0Normalized Return
Pendulum-v1
0 2 4
Steps 1e8
0.0
0.2
0.4
0.6
0.8
1.0Normalized Return
Brax Ant
0 2 4
Steps 1e8
0.0
0.2
0.4
0.6
0.8
1.0Normalized Return
Brax Humanoid
0 2 4
Steps 1e8
0.0
0.2
0.4
0.6
0.8
1.0
1.2Normalized Return
Brax Inverted Double Pendulum
0 2 4
Steps 1e8
0.2
0.4
0.6
0.8
1.0Normalized Return
Craftax Classic
0 2 4
Steps 1e8
0.0
0.2
0.4
0.6
0.8
1.0Normalized Return
Craftax Symbolic
0 2 4
Steps 1e8
0.0
0.2
0.4
0.6
0.8
1.0Normalized Return
Jumanji 2048
0 2 4
Steps 1e8
0.2
0.4
0.6
0.8
1.0Normalized Return
Jumanji Knapsack
0 2 4
Steps 1e8
0.0
0.2
0.4
0.6
0.8
1.0Normalized Return
Jumanji Snake
0 2 4
Steps 1e8
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
Normalized Return
Kinetix Hard Pinball (l)
0 2 4
Steps 1e8
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75Normalized Return
Kinetix Thrust Control Left (m)
0 2 4
Steps 1e8
0.96
0.98
1.00
1.02
1.04Normalized Return
Kinetix Thrust Over Ball (s)
0 2 4
Steps 1e8
0.0
0.2
0.4
0.6
0.8
1.0Normalized Return
Navix DoorKey (8x8)
0 2 4
Steps 1e8
0.2
0.4
0.6
0.8
1.0Normalized Return
Navix Dynamic Obstacles Random (6x6)
0 2 4
Steps 1e8
0.0
0.2
0.4
0.6
0.8
1.0Normalized Return
Navix FourRooms (8x8)
Figure 9: Comparison of reinforcement learning results: Mean returns for each environment and algorithm across 10 random
seeds. The returns are evaluated using the mean of the parameters. HPO was conducted for each algorithm/environment
pair. The shaded region is the standard error of the mean.
G.2 Multi Agent Reinforcement Learning Experiments
We train on three cooperative Multi Particle Environments (MPEs) (Lowe et al., 2017) implemented in
JaxMARL (Rutherford et al., 2023) with feed-forward networks of width 64 and depth 3, performing Bayesian
hyperparameter optimization for each environment and algorithm. All runs were executed on NVIDIA
A100-SXM4-40GB GPUs. We find that the optimal batch size is consistent across algorithms on the same
environment. Figure 11 shows that EGGROLL with rank 1 trains up to 2.4 times faster than OpenES for large
batch sizes while staying competitive in performance.
39
EggRoll OpenES
0
100
200
300
400Time (seconds)
1.83x faster
CartPole-v1
EggRoll OpenES
0
100
200
300Time (seconds)
7.81x faster
Pendulum-v1
EggRoll OpenES
0
5000
10000Time (seconds)
2.47x faster
Brax Ant
EggRoll OpenES
0
2500
5000
7500
10000Time (seconds)
1.80x slower
Brax Humanoid
EggRoll OpenES
0
1000
2000
3000Time (seconds)
1.65x slower
Brax Inverted Double Pendulum
EggRoll OpenES
0
5000
10000
15000Time (seconds)
1.60x faster
Craftax Classic
EggRoll OpenES
0
10000
20000
30000Time (seconds)
1.29x slower
Craftax Symbolic
EggRoll OpenES
0
200
400
600Time (seconds)
5.26x faster
Jumanji 2048
EggRoll OpenES
0
500
1000
1500Time (seconds)
11.17x faster
Jumanji Knapsack
EggRoll OpenES
0
20000
40000Time (seconds)
40.68x faster
Jumanji Snake
EggRoll OpenES
0
5000
10000
15000Time (seconds)
28.54x faster
Kinetix Hard Pinball (l)
EggRoll OpenES
0
1000
2000
3000
4000Time (seconds)
1.60x faster
Kinetix Thrust Control Left (m)
EggRoll OpenES
0
1000
2000
3000
4000Time (seconds)
1.96x slower
Kinetix Thrust Over Ball (s)
EggRoll OpenES
0
200
400Time (seconds)
1.61x faster
Navix DoorKey (8x8)
EggRoll OpenES
0
1000
2000
3000Time (seconds)
3.14x faster
Navix Dynamic Obstacles Random (6x6)
EggRoll OpenES
0
500
1000
1500Time (seconds)
2.25x faster
Navix FourRooms (8x8)
Figure 10: Comparison of reinforcement learning results: Mean and standard deviation of training time.
Table 1: Hyperparameter Ranges for EGGROLL and OpenES
Hyperparameter Values
pop_size 512, 1024, 2048, 4096
n_parallel_evaluations 1, 4, 8
rank 1, 2, 4
optimizer adamw, sgd, adam
learning_rate 1e-3, 1e-2, 1e-1
lr_decay 0.995, 0.999, 0.9995, 1.0
sigma 0.05, 0.2, 0.5
sigma_decay 0.995, 0.999, 0.9995, 1.0
rank_transform true, false
deterministic_policy true, false
40
Table 2: Hyperparameter Ranges for PPO
Hyperparameter Values
clip_eps 0.1, 0.2, 0.3
ent_coef 0, 0.0001, 0.001
gae_lambda 0.9, 0.95, 0.98
gamma 0.95, 0.99, 0.995, 0.999
learning_rate 0.0001, 0.0003, 0.001
max_grad_norm 0.5, 1, 2
layer_size 256
n_layers 3
normalize_observations true
normalize_rewards false
num_envs 64, 128, 256
num_epochs 4, 8, 16
num_minibatches 16, 32, 64
num_steps 64, 128, 256
reward_normalization_discount 0.99
skip_initial_evaluation false
vf_coef 0.5, 0.75, 1
Table 3: CartPole-v1
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false true
learning_rate 0.1 0.1
lr_decay 0.9995 0.9995
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 1 4
pop_size 2048 512
optimizer sgd adamw
rank 4 /
rank_transform false true
sigma 0.2 0.5
sigma_decay 0.999 0.9995
Table 4: Pendulum-v1
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false true
learning_rate 0.01 0.01
lr_decay 0.995 0.995
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 1 4
pop_size 4096 4096
optimizer adam adamw
rank 4 /
rank_transform false false
sigma 0.05 0.05
sigma_decay 0.995 1
G.3 Reasoning Fine-tuning Experiments: Countdown
We ran a Bayesian hyper-parameter sweep (Snoek et al., 2012) for both GRPO and EGGROLL and used
the best set found to run the experiments in figure 5a. For GRPO we swept over sampling temperature and
learning rate, whereas for EGGROLL we swept over the standard deviation of the ES sampling (σ) and the
learning rate scale. The best hyper-parameters found are detailed on tables 26 (EGGROLL) and 27 (GRPO).
All of the experiments run in 8 hours on a NVIDIA H200 GPU.
We also run an experiment where we increase the number of GPUs to 8 and use a bigger model, RWKV 7g7B,
on the Countdown task, allowing for stronger final performance. Notably, we compare to the results reported
by Qiu et al. (2025) on Countdown. Figure 12 shows that starting from our significantly weaker model (RWKV
7g7B v.s. Qwen 2.5-7B), we are able to train to a higher validation accuracy (72.9%), v.s. the ones reported
for training with GRPO (52.8%) and Open ES (66.8%). Qiu et al. (2025) do not report the wall clock time or
the hardware used for their experiments which makes it difficult to establish a fair comparison.
41
Table 5: brax/ant
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false false
learning_rate 0.01 0.1
lr_decay 0.9995 0.995
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 1 8
pop_size 2048 512
optimizer adam adam
rank 1 /
rank_transform false false
sigma 0.05 0.05
sigma_decay 0.9995 0.9995
Table 6: brax/humanoid
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy true false
learning_rate 0.1 0.1
lr_decay 1 0.995
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 8 8
pop_size 4096 1024
optimizer adam sgd
rank 1 /
rank_transform true true
sigma 0.2 0.2
sigma_decay 0.9995 0.995
Table 7: brax/inverted_double_pendulum
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy true true
learning_rate 0.1 0.1
lr_decay 1 0.995
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 1 1
pop_size 2048 4096
optimizer adam adam
rank 2 /
rank_transform true true
sigma 0.5 0.05
sigma_decay 0.995 1
Table 8: craftax/Craftax-Classic-Symbolic-AutoReset-v1
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false false
learning_rate 0.01 0.001
lr_decay 0.995 0.995
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 4 8
pop_size 2048 4096
optimizer sgd adamw
rank 1 /
rank_transform false false
sigma 0.05 0.05
sigma_decay 1 0.995
Table 9: craftax/Craftax-Symbolic-AutoReset-v1
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false false
learning_rate 0.01 0.1
lr_decay 0.999 0.995
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 1 4
pop_size 512 1024
optimizer sgd adam
rank 4 /
rank_transform true false
sigma 0.05 0.5
sigma_decay 0.999 1
Table 10: jumanji/Game2048-v1
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false true
learning_rate 0.1 0.01
lr_decay 1 0.999
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 4 4
pop_size 1024 1024
optimizer adamw adamw
rank 1 /
rank_transform false true
sigma 0.5 0.05
sigma_decay 0.9995 0.9995
42
Table 11: jumanji/Knapsack-v1
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false false
learning_rate 0.1 0.01
lr_decay 0.999 1
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 4 1
pop_size 1024 2048
optimizer sgd adamw
rank 4 /
rank_transform true true
sigma 0.05 0.5
sigma_decay 1 0.995
Table 12: jumanji/Snake-v1
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false false
learning_rate 0.001 0.001
lr_decay 0.9995 1
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 8 1
pop_size 4096 2048
optimizer adam sgd
rank 1 /
rank_transform true false
sigma 0.05 0.2
sigma_decay 0.9995 1
Table 13: kinetix/l/hard_pinball
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy true true
learning_rate 0.01 0.01
lr_decay 0.995 1
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 8 1
pop_size 2048 512
optimizer sgd sgd
rank 4 /
rank_transform true true
sigma 0.05 0.5
sigma_decay 0.999 0.9995
Table 14: kinetix/m/h17_thrustcontrol_left
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false false
learning_rate 0.1 0.001
lr_decay 0.9995 1
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 4 1
pop_size 512 1024
optimizer sgd adam
rank 4 /
rank_transform true true
sigma 0.5 0.5
sigma_decay 1 0.999
Table 15: kinetix/s/h1_thrust_over_ball
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false false
learning_rate 0.1 0.01
lr_decay 0.995 0.995
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 1 1
pop_size 512 2048
optimizer adamw sgd
rank 1 /
rank_transform true true
sigma 0.5 0.05
sigma_decay 0.9995 1
Table 16: navix/Navix-DoorKey-8x8-v0
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false false
learning_rate 0.01 0.01
lr_decay 0.9995 1
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 1 8
pop_size 1024 2048
optimizer adamw adam
rank 1 /
rank_transform false true
sigma 0.05 0.05
sigma_decay 1 1
G.4 Reasoning Fine-tuning Experiments: GSM8K
We used the hyper-parameters found for Countdown as a starting point and reduced the learning rates for
both GRPO and EGGROLL using linear search until we found the best performing one on the validation set.
43
Table 17: navix/Navix-Dynamic-Obstacles-6x6-Random-
v0
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false false
learning_rate 0.01 0.01
lr_decay 0.999 1
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 4 1
pop_size 512 4096
optimizer adam adam
rank 2 /
rank_transform false false
sigma 0.05 0.2
sigma_decay 1 0.995
Table 18: navix/Navix-FourRooms-v0
Hyperparameter eggroll open_es
activation pqn pqn
deterministic_policy false false
learning_rate 0.01 0.001
lr_decay 0.999 0.9995
layer_size 256 256
n_layers 3 3
n_parallel_evaluations 4 4
pop_size 2048 2048
optimizer sgd adam
rank 4 /
rank_transform true false
sigma 0.05 0.05
sigma_decay 0.9995 0.9995
Table 19: PPO Hyperparameters (Set 1)
Hyperparameter CartPole Pendulum Ant Humanoid IDP CraftaxClassic CraftaxSymbolic Game2048
activation pqn pqn pqn pqn pqn pqn pqn pqn
clip_eps 0.2 0.1 0.2 0.3 0.1 0.2 0.2 0.3
ent_coef 0.0001 0.001 0 0.0001 0.0001 0.0001 0 0.001
gae_lambda 0.9 0.95 0.95 0.9 0.98 0.98 0.9 0.9
gamma 0.995 0.999 0.995 0.95 0.99 0.95 0.95 0.99
learning_rate 0.0003 0.0003 0.0003 0.0001 0.001 0.001 0.0003 0.0003
max_grad_norm 0.5 1 0.5 2 2 2 2 2
layer_size 256 256 256 256 256 256 256 256
n_layers 3 3 3 3 3 3 3 3
normalize_obs true true true true true true true true
normalize_rew false false false false false false false false
num_envs 256 256 64 256 64 128 256 64
num_epochs 4 16 8 4 4 4 4 8
num_minibatches 32 16 32 64 64 32 32 16
num_steps 128 256 128 64 128 128 64 64
rew_norm_discount 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99
skip_initial_eval false false false false false false false false
vf_coef 0.5 1 1 0.75 1 0.5 0.75 0.75
Our experiments for GSM8K run on 8 NVIDIA H200 GPUS for 8 hours each. We also increase the standard
deviation, σ, parameter for ES (from 7×10 −4 to 2×10 −3) as the significantly bigger population sizes (8096
v.s. 512) allow for much more stable training and aggressive exploration.
44
Table 20: PPO Hyperparameters (Set 2)
Hyperparameter Knapsack Snake HardPinball ThrustLeft ThrustBall DoorKey DynamicObs FourRooms
activation pqn pqn pqn pqn pqn pqn pqn pqn
clip_eps 0.1 0.3 0.1 0.2 0.2 0.1 0.1 0.1
ent_coef 0.0001 0.001 0.0001 0.0001 0.0001 0.0001 0.001 0.001
gae_lambda 0.9 0.95 0.9 0.9 0.95 0.98 0.98 0.9
gamma 0.99 0.999 0.99 0.995 0.999 0.95 0.999 0.99
learning_rate 0.0001 0.0001 0.0001 0.0001 0.0001 0.0003 0.001 0.001
max_grad_norm 0.5 0.5 1 2 0.5 0.5 1 1
layer_size 256 256 256 256 256 256 256 256
n_layers 3 3 3 3 3 3 3 3
normalize_obs true true true true true true true true
normalize_rew false false false false false false false false
num_envs 256 128 256 256 64 64 128 256
num_epochs 4 4 16 16 16 16 4 8
num_minibatches 64 16 16 32 16 64 16 32
num_steps 128 128 64 128 64 256 128 256
rew_norm_discount 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99
skip_initial_eval false false false false false false false false
vf_coef 0.75 0.75 0.5 0.5 0.5 0.75 0.5 0.75
0 2 4
Steps 1e8
−80
−60
−40Return
Simple Spread
0 2 4
Steps 1e8
−150
−100
−50
Simple Speaker Listener
0 2 4
Steps 1e8
−60
−50
−40
Simple Reference
IPPO
OpenES
EGGROLL
IPPO OpenES EGGROLL
(Batch Size = 128)
0
5
10
15Wall Clock Time (mins)
IPPO OpenES EGGROLL
(Batch Size = 512)
0
1
2
3
4
5
IPPO OpenES EGGROLL
(Batch Size = 4096)
0.0
0.5
1.0
1.5
2.0
Figure 11: Training curves and wall clock times for cooperative Multi Particle Environments. Hyperparameter optimization
yielded equal batch sizes for all algorithms on the same environment. All EGGROLL runs used rank 1 perturbations.
Shaded regions are standard errors of mean values.
Table 21: Hyperparameter Ranges Used in MPE Sweeps
for EGGROLL and OpenES
Hyperparameter Values
activation pqn, tanh
pop_size 128, 512, 1024, 2048, 4096
learning_rate 0.01, 0.05, 0.1, 0.5
lr_decay 0.3, 0.7, 1.0
sigma 0.1, 0.2, 0.3, 0.4, 0.5
rank_transform true, false
Table 22: Hyperparameter Ranges Used in MPE Sweeps
for IPPO
Hyperparameter Values
activation relu, tanh
pop_size 128, 512, 1024, 2048, 4096
learning_rate 5e-5, 1e-4, 2.5e-4, 1e-3
entropy_coef 0.001, 0.005, 0.01
45
Table 23: MPE Simple Spread v3
Hyperparameter eggroll open_es ippo
activation tanh tanh tanh
deterministic_policy true true false
learning_rate 0.01 0.01 0.001
lr_decay 0.7 0.7 linear
layer_size 64 64 64
n_layers 3 3 3
pop_size 128 128 128
optimizer adamw adamw adam
rank 1 1 -
rank_transform false false -
sigma 0.5 0.5 -
n_minibatches - - 4
update_epochs - - 4
gamma - - 0.99
gae_lambda - - 0.95
epsilon_clip - - 0.2
entropy_coef - - 0.01
value_coef - - 0.5
max_grad_norm - - 0.5
Table 24: MPE Simple Speaker Listener v4
Hyperparameter eggroll open_es ippo
activation tanh tanh relu
deterministic_policy true true false
learning_rate 0.01 0.01 0.001
lr_decay 0.7 0.3 linear
layer_size 64 64 64
n_layers 3 3 64
pop_size 512 512 512
optimizer adamw adamw adam
rank 1 1 -
rank_transform true true -
sigma 0.5 0.5 -
n_minibatches - - 4
update_epochs - - 4
gamma - - 0.99
gae_lambda - - 0.95
epsilon_clip - - 0.2
entropy_coef - - 0.005
value_coef - - 0.5
max_grad_norm - - 0.5
Table 25: MPE Simple Reference v3
Hyperparameter eggroll open_es ippo
activation pqn tanh relu
deterministic_policy true true false
learning_rate 0.01 0.01 0.001
lr_decay 0.3 0.3 linear
layer_size 64 64 64
n_layers 3 3 3
pop_size 4096 4096 4096
optimizer adamw adamw adam
rank 1 1 -
rank_transform false true -
sigma 0.1 0.3 -
n_minibatches - - 4
update_epochs - - 4
gamma - - 0.99
gae_lambda - - 0.95
epsilon_clip - - 0.2
entropy_coef - - 0.01
value_coef - - 0.5
max_grad_norm - - 0.5
46
Hyperparameter Value
Model RWKV 7g1.5B
Optimizer Gradient descent
ES standard deviationσ7×10 −4
Rankr1
Learning-rate scaleηscale 0.125
Population size 256
Parallel generations per GPU 1536
Prompts per epoch 6
Generation / thinking length 1000 tokens
Train / val temperature 0 / 0
Parallel validations 128
Table 26: Key hyperparameters for EGGROLL training on Countdown with FastRWKV-7g1.5B.
Hyperparameter Value
Model RWKV 7g1.5B
Optimizer Radam
Learning rateη3×10 −6
Generations per promptG8
Parallel generations per GPU 64
Prompts per epoch 8
Generation length 1000 tokens
Number of minibatches 4
PPO clip parameterϵclip 0.2
Train / val temperature 1 / 0
Parallel validations 128
Table 27: Key hyperparameters for GRPO training on Countdown with AssociativeScanRWKV-7g1.5B.
0 100 200 300 400 500
Epoch
0.2
0.3
0.4
0.5
0.6
0.7Validation Score
  Original: Qwen-2.5-7B (0.312)
  GRPO: Qwen-2.5-7B (0.528)
  OpenES: Qwen-2.5-7B (0.668)
EGGROLL: RWKV-g0:7g7B
Figure 12: Validation score training curve of an RWKV 7g7B model on countdown using 8 GPUS and a population size of
8096. Notably, we are able to outperform a stronger base model trained with GRPO and OpenES.
47
Hyperparameter Value
Model RWKV 7g7B
ES standard deviationσ2×10 −3
Rankr1
Learning-rate scaleηscale 0.06
Generations per promptG512
Parallel generations per GPU 1024
Total parallel generations 8192
Prompts per epoch 16
Generation length 1000 tokens
Noise reuse factor 1
Freeze non-LoRA params True
Train / val temperature 0 / 0
Parallel validations 128
Table 28: Key hyperparameters for multi-GPU EGGROLL training on GSM8K with FastRWKV-7g7B.
Hyperparameter Value
Model RWKV 7g7B
Learning rateη1×10 −6
Generations per promptG8
Parallel generations per GPU 32
Total parallel generations 256
Prompts per epoch 32
Generation length 1000 tokens
Number of minibatches 16
Number of workers (processes) 8
PPO clip parameterϵclip 0.2
Train / val temperature 1 / 0
Parallel validations 128
Table 29: Key hyperparameters for multi-GPU GRPO training on GSM8K with AssociativeScanRWKV-7g7B.
48
